{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#pip install recommenders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(tf.executing_eagerly())  # should be True now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import seaborn as sns\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import ast\n",
    "import re\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout,Attention,Multiply,GlobalAveragePooling1D\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from math import sqrt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from time import time\n",
    "#from recommenders.evaluation.python_evaluation import (\n",
    "    #map, ndcg_at_k, precision_at_k, recall_at_k\n",
    "#)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict,Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 : LOADING THE DATASET FROM KAGGLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "devices = tf.config.get_visible_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#path = kagglehub.dataset_download(\"samlearner/letterboxd-movie-ratings-data\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#movie_data_filename = 'movie_data.csv'\n",
    "#ratings_filename = 'ratings_export.csv'\n",
    "#users_filename = 'users_export.csv'\n",
    "#movie_data_path = os.path.join(path,movie_data_filename)\n",
    "#ratings_path = os.path.join(path,ratings_filename)\n",
    "#users_path = os.path.join(path,users_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# movie_data_df = pd.read_csv(movie_data_path,engine='python')\n",
    "# ratings_df = pd.read_csv(ratings_path,engine='python')\n",
    "# users_df = pd.read_csv(users_path,engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = pd.read_parquet('/kaggle/input/parquet-dataset/movie_data.parquet',engine='pyarrow')\n",
    "ratings_df = pd.read_parquet('/kaggle/input/parquet-dataset/ratings_data.parquet',engine='pyarrow')\n",
    "users_df = pd.read_parquet('/kaggle/input/parquet-dataset/users_dt.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_genre_string(genre_str):\n",
    "    # If the value is not a string (it could be float), convert to '[]'\n",
    "    if not isinstance(genre_str, str):\n",
    "        genre_str = '[]'\n",
    "    # Now attempt parsing\n",
    "    try:\n",
    "        parsed = ast.literal_eval(genre_str)\n",
    "        # If parsing somehow doesn't return a list, force it\n",
    "        if not isinstance(parsed, list):\n",
    "            return []\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df['genres'] = movie_data_df['genres'].fillna('[]')\n",
    "movie_data_df['genres'] = movie_data_df['genres'].apply(parse_genre_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(movie_data_df['genres'].head(50))\n",
    "#print(movie_data_df['genres'].tail(50))\n",
    "#print(movie_data_df['genres'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_genres1 = set()\n",
    "for g_list in movie_data_df['genres']:\n",
    "    all_genres1.update(g_list)\n",
    "\n",
    "print(\"Unique genres found:\", all_genres1)\n",
    "print(\"Number of unique genres:\", len(all_genres1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(users_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this dataframe won't be used, as we don't want the models predictions to depend on the users data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unneccessary columns need to be removed - image url, imdb id, imdb, link (perhaps needed to enhance model with imdb data ? ), tmbd id, link, (download that dataset and enhance with it ? )production countries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we can check if adding additional information from this dataframe to the model improves its performance (genre of the movie, overview? tokenized, year_released)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#movie_data_df.to_parquet('movie_data.parquet', engine='pyarrow')\n",
    "#ratings_df.to_parquet('ratings_data.parquet', engine = 'pyarrow')\n",
    "#users_df.to_parquet('users_dt.parquet',engine = 'pyarrow')\n",
    "# Load from Parquet file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: EXPLORATORY DATA ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = movie_data_df.dropna(subset=['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df['overview_length'] = movie_data_df['overview'].apply(len)\n",
    "\n",
    "avg_length = movie_data_df['overview_length'].mean()\n",
    "min_length = movie_data_df['overview_length'].min()\n",
    "max_length = movie_data_df['overview_length'].max()\n",
    "movie_data_df.drop(['overview_length'],axis = 1)\n",
    "print(f\"Average Overview Length: {avg_length:.2f}\")\n",
    "print(f\"Minimum Overview Length: {min_length}\")\n",
    "print(f\"Maximum Overview Length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df.dropna(subset=['user_id', 'movie_id', 'rating_val'], inplace=True)\n",
    "movie_data_df.dropna(subset=['genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_mapping_movie_data = {user : idx for idx, user in enumerate(movie_data_df['movie_id'].unique())}\n",
    "user_mapping = {user: idx for idx, user in enumerate(ratings_df['user_id'].unique())}\n",
    "item_mapping = {item: idx for idx, item in enumerate(ratings_df['movie_id'].unique())}\n",
    "all_genres = set(genre for genres in movie_data_df['genres'] for genre in genres)\n",
    "genre_mapping = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "movie_data_df['genre_mapped'] = movie_data_df['genres'].apply(\n",
    "     lambda genres: [genre_mapping[genre] for genre in genres if genre in genre_mapping]\n",
    " )\n",
    "print(genre_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_items = list(user_mapping.items())[:10]\n",
    "print(sample_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_items1 = list(user_mapping_movie_data.items())[:10]\n",
    "print(sample_items1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df['movie_id'] = movie_data_df['movie_id'].map(user_mapping_movie_data)\n",
    "print(movie_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df['user_id'] = ratings_df['user_id'].map(user_mapping)\n",
    "ratings_df['movie_id'] = ratings_df['movie_id'].map(item_mapping)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non integer frames must be mapped to a unique numeric value - movie id and user id in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.drop(['_id'],axis = 1)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_users = len(user_mapping)\n",
    "n_items = len(item_mapping)\n",
    "print(f'No. users : {n_users}, no. items : {n_items}')\n",
    "#print(user_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df['rating_val'] = ratings_df['rating_val'].astype(int)  \n",
    "\n",
    "# Count the number of ratings per movie\n",
    "rating_counts = ratings_df.groupby('movie_id')['rating_val'].count()\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(rating_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Number of Ratings per Movie\")\n",
    "plt.ylabel(\"Count of Movies\")\n",
    "plt.title(\"Distribution of Number of Ratings per Movie\")\n",
    "plt.yscale('log') \n",
    "plt.show()\n",
    "plt.savefig('rating_freq.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_agg = ratings_df.groupby('movie_id').agg(\n",
    "    avg_rating=('rating_val', 'mean'),\n",
    "    num_ratings=('rating_val', 'count')\n",
    ").reset_index()\n",
    "\n",
    "movie_data_df['movie_id'] = movie_data_df['movie_id'].astype(int)  \n",
    "merged_df = pd.merge(movie_data_df, ratings_agg, on='movie_id', how='inner')\n",
    "\n",
    "# Optional: Convert popularity to numeric if necessary\n",
    "merged_df['popularity'] = pd.to_numeric(merged_df['popularity'], errors='coerce')\n",
    "\n",
    "# Step 3: Create the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    merged_df['popularity'],\n",
    "    merged_df['avg_rating'],\n",
    "    s=merged_df['num_ratings'] * 0.1, \n",
    "    c=merged_df['vote_average'],       \n",
    "    cmap='viridis',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Relationship between Movie Popularity and Average Rating')\n",
    "plt.colorbar(scatter, label='Vote Average')\n",
    "plt.xscale('log') \n",
    "plt.grid(True)\n",
    "plt.savefig('popularity_vs_rating.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: DEFINING THE TASK, EVALUATION METRICS, CONSTRUCTING MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ncf_model.load_weights('ncf_model.weights.h5')\n",
    "# model can be saved locally after training so it doesn't have to be redone again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, val = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "train, test = train_test_split(train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointwise loss prefered in ranking systems. Need to construct triple of user,pos item, neg item, and train to model to successfully rate the pos item as high as possible. (previosly was done with pointwise loss and biased towards best rated movies ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "threshold = 8  \n",
    "num_items = 286069  # total num of movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate training triples: (user, pos_item, neg_item)\n",
    "def generate_training_triples(ratings_df, num_items, threshold,user_pos):\n",
    "    user_list, pos_item_list, neg_item_list = [], [], []\n",
    "    \n",
    "    for user, pos_items in tqdm(user_pos.items(), total=len(user_pos), desc=\"Processing users\"):\n",
    "        for pos_item in pos_items:\n",
    "            # Sample a negative item\n",
    "            # We'll sample until we find an item the user hasn't interacted with positively.\n",
    "            while True:\n",
    "                neg_item = np.random.randint(0, num_items)\n",
    "                if neg_item not in pos_items:\n",
    "                    break\n",
    "            user_list.append(user)\n",
    "            pos_item_list.append(pos_item)\n",
    "            neg_item_list.append(neg_item)\n",
    "            \n",
    "    return np.array(user_list), np.array(pos_item_list), np.array(neg_item_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "positive_df = ratings_df['rating_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_pos = ratings_df.groupby('user_id')['movie_id'].agg(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "users_all, pos_items_all, neg_items_all = generate_training_triples(ratings_df,num_items,threshold,user_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "users_train, users_temp, pos_items_train, pos_items_temp, neg_items_train, neg_items_temp = train_test_split(\n",
    "    users_all, pos_items_all, neg_items_all,\n",
    "    test_size=0.4,  # 40% goes to temp (validation+test)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then, split the temporary set equally into validation and test sets\n",
    "users_val, users_test, pos_items_val, pos_items_test, neg_items_val, neg_items_test = train_test_split(\n",
    "    users_temp, pos_items_temp, neg_items_temp,\n",
    "    test_size=0.5,  # 50% of temp for validation, 50% for test (i.e., 20% of original each)\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(user_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(neg_item_train.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easiest to construct model using functional API (multiple input network). 3 dense layers of sizes 64,32,16 respectively follow the embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise loss model - bayesian personalized ranking loss function as per the paper BPR: Bayesian Personalized Ranking from Implicit Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 32  # Latent factor size\n",
    "mlp_layer_sizes = [64, 32, 16]  # Fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bpr_loss(_, y_pred):\n",
    "    pos_score, neg_score = y_pred[:, 0], y_pred[:, 1]\n",
    "    loss = -K.mean(K.log(K.sigmoid(pos_score - neg_score)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# user_train, pos_item_train, neg_item_train define needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_users = 7477\n",
    "embedding_dim = 32\n",
    "mlp_layer_sizes = [64, 32, 16, 8]\n",
    "\n",
    "user_embedding_layer = Embedding(num_users, embedding_dim, name='user_embedding')\n",
    "item_embedding_layer = Embedding(num_items, embedding_dim, name='item_embedding')\n",
    "\n",
    "def scoring_model():\n",
    "    user_in = Input(shape=(1,), name='user_in')\n",
    "    item_in = Input(shape=(1,), name='item_in')\n",
    "    \n",
    "    user_vec = Flatten()(user_embedding_layer(user_in))\n",
    "    item_vec = Flatten()(item_embedding_layer(item_in))\n",
    "    \n",
    "    gmf_vec = Multiply()([user_vec, item_vec])\n",
    "    \n",
    "    concat_vec = Concatenate()([user_vec, item_vec])\n",
    "    mlp = concat_vec\n",
    "    for size in mlp_layer_sizes:\n",
    "        mlp = Dense(size, activation='relu')(mlp)\n",
    "        mlp = Dropout(0.2)(mlp)\n",
    "    \n",
    "    pre_output = Concatenate()([gmf_vec, mlp])\n",
    "    output = Dense(1, activation='linear')(pre_output)\n",
    "    \n",
    "    return Model(inputs=[user_in, item_in], outputs=output, name='scoring_model')\n",
    "\n",
    "scoring_net = scoring_model()\n",
    "scoring_net.summary()\n",
    "\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "pos_item_input = Input(shape=(1,), name='pos_item_input')\n",
    "neg_item_input = Input(shape=(1,), name='neg_item_input')\n",
    "\n",
    "pos_score = scoring_net([user_input, pos_item_input])\n",
    "neg_score = scoring_net([user_input, neg_item_input])\n",
    "\n",
    "y_pred = Concatenate(axis=1, name='y_pred')([pos_score, neg_score])\n",
    "\n",
    "pairwise_model = Model(inputs=[user_input, pos_item_input, neg_item_input], outputs=y_pred)\n",
    "pairwise_model.summary()\n",
    "\n",
    "def bpr_loss(_, y_pred):\n",
    "    pos_score = y_pred[:, 0]\n",
    "    neg_score = y_pred[:, 1]\n",
    "    loss = -K.mean(K.log(K.sigmoid(pos_score - neg_score)))\n",
    "    return loss\n",
    "\n",
    "pairwise_model.compile(optimizer='adam', loss=bpr_loss)\n",
    "\n",
    "dummy_y = np.zeros((neg_items_train.size, 2))\n",
    "dummy_y_val = np.zeros((neg_items_val.size,2))\n",
    "#pairwise_model.load_weights('/kaggle/input/pairwise_weights/tensorflow2/v1/1/pairwise_model_all.weights(1).h5')\n",
    "pairwise_model.fit([users_train, pos_items_train, neg_items_train],\n",
    "        dummy_y,\n",
    "        validation_data=(\n",
    "        [users_val, pos_items_val, neg_items_val],    \n",
    "            dummy_y_val\n",
    "    ),  epochs=3, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pairwise_model.save_weights('pairwise_model_all.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(pos_items_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"pos_items_test shape:\", pos_items_test.shape)\n",
    "print(\"neg_items_test shape:\", neg_items_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_items_set = set(ratings_df['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dict = defaultdict(set)\n",
    "\n",
    "# Loop over the training examples to build the dictionary\n",
    "for user, item in zip(users_train, pos_items_train):\n",
    "    train_dict[user].add(item)\n",
    "\n",
    "# If you want a regular dictionary (instead of defaultdict), convert it:\n",
    "train_dict = dict(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_user_ids = ratings_df['user_id'].unique()\n",
    "\n",
    "\n",
    "k = 10\n",
    "# Lists for evaluation metrics\n",
    "hits = []\n",
    "ndcgs = []\n",
    "all_true_labels = []\n",
    "all_pred_scores = []\n",
    "recommended_items = []\n",
    "\n",
    "for user_id in tqdm(unique_user_ids, desc=\"Evaluating users\"):\n",
    "    # Get positive items for this user (from your pos arrays)\n",
    "    pos_indices = np.where(users_test == user_id)[0]\n",
    "    user_pos_items = pos_items_test[pos_indices]\n",
    "    \n",
    "    # Generate negative samples for the user:\n",
    "    # Here we sample negatives from items that are NOT in the training set for the user.\n",
    "    # You can adjust the number of negatives per positive as needed.\n",
    "    n_neg_samples = 99\n",
    "    negatives = []\n",
    "    # Ensure that we sample enough negatives\n",
    "    possible_negatives = list(all_items_set - train_dict.get(user_id, set()))\n",
    "    if len(possible_negatives) < n_neg_samples:\n",
    "        negatives = possible_negatives\n",
    "    else:\n",
    "        negatives = random.sample(possible_negatives, n_neg_samples)\n",
    "    \n",
    "    # For each positive item, create a test batch with the positive and the negative items.\n",
    "    # Alternatively, if you want to evaluate the user as a whole, you could combine all positive items and negatives.\n",
    "    # Here we demonstrate combining them for a single evaluation round.\n",
    "    test_items = np.concatenate([user_pos_items, negatives])\n",
    "    # Create an array of the same user id for each item\n",
    "    user_batch = np.full(test_items.shape, user_id)\n",
    "    \n",
    "    # Predict scores using your scoring model (which outputs one score per (user, item) pair)\n",
    "    scores = scoring_net.predict([user_batch, test_items], verbose=0)\n",
    "    scores = np.squeeze(scores)  # Ensure it's 1D\n",
    "    \n",
    "    # Create labels: 1 for each positive, 0 for each negative.\n",
    "    labels = np.concatenate([np.ones(len(user_pos_items)), np.zeros(len(negatives))])\n",
    "    \n",
    "    # For overall AUC\n",
    "    all_true_labels.extend(labels)\n",
    "    all_pred_scores.extend(scores)\n",
    "    \n",
    "    # Rank items based on scores\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "    sorted_items = test_items[sorted_indices]\n",
    "    \n",
    "    # Record top-k items for analysis\n",
    "    top_k_items = sorted_items[:k]\n",
    "    recommended_items.extend(top_k_items.tolist())\n",
    "    \n",
    "    # Compute Hit@k: Check if any positive is in the top k\n",
    "    if 1 in sorted_labels[:k]:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "    \n",
    "    # Compute NDCG@k: Discounted gain for the first positive\n",
    "    pos_ranks = np.where(sorted_labels[:k] == 1)[0]\n",
    "    if pos_ranks.size > 0:\n",
    "        ndcgs.append(1 / math.log2(pos_ranks[0] + 2))\n",
    "    else:\n",
    "        ndcgs.append(0)\n",
    "\n",
    "# Compute final metrics\n",
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "print(f\"Hit Rate@{k}: {hit_rate:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit Rate@10: 0.9920\n",
    "### NDCG@10: 0.9879\n",
    "### AUC: 0.9556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_pred_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('pairwise_loss_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_id = 'jay'  # Replace with a valid user identifier as in your mapping\n",
    "user_idx = user_mapping.get(user_id)\n",
    "\n",
    "all_items = np.arange(n_items)  \n",
    "user_tensor = np.array([user_idx] * n_items)  \n",
    "\n",
    "# Get predicted scores using the scoring network.\n",
    "predicted_ratings = scoring_net.predict([user_tensor, all_items], verbose=0)\n",
    "\n",
    "# Flatten and sort predictions to get the top recommendations.\n",
    "top_items = np.argsort(predicted_ratings.flatten())[::-1][:10]\n",
    "\n",
    "# Map back the indices to movie IDs (or names). \n",
    "# This reverse mapping depends on how item_mapping is defined.\n",
    "# One way is to create a reverse lookup dictionary:\n",
    "reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "recommended_movie_ids = [reverse_item_mapping[idx] for idx in top_items]\n",
    "\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movie_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_id = 'jay'  \n",
    "user_idx = user_mapping.get(user_id)\n",
    "print(\"User index:\", user_idx)\n",
    "\n",
    "all_items = np.arange(n_items)\n",
    "\n",
    "user_seen_items = train_dict.get(user_idx, set())\n",
    "\n",
    "candidate_items = np.array([item for item in all_items if item not in user_seen_items])\n",
    "\n",
    "print(f\"Number of candidate items for user {user_id}: {len(candidate_items)}\")\n",
    "\n",
    "user_tensor = np.full(candidate_items.shape, user_idx)\n",
    "\n",
    "predicted_scores = scoring_net.predict([user_tensor, candidate_items], verbose=0)\n",
    "predicted_scores = predicted_scores.flatten()\n",
    "\n",
    "top_indices = np.argsort(predicted_scores)[::-1][:10]\n",
    "top_candidate_items = candidate_items[top_indices]\n",
    "\n",
    "reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "recommended_movie_ids = [reverse_item_mapping[idx] for idx in top_candidate_items]\n",
    "\n",
    "print(f\"Recommended movies for user {user_id} (only from unseen items): {recommended_movie_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with pointwise loss function (mean squared error) - shows significant bias towards higher rated movies in the reccomendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "user_embedding = Embedding(n_users, embedding_dim, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_items, embedding_dim, name='item_embedding')(item_input)\n",
    "\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "\n",
    "gmf_vec = Multiply()([user_vec,item_vec])\n",
    "concat_vec = Concatenate()([user_vec, item_vec])\n",
    "mlp = concat_vec\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp = Dense(size, activation='relu')(mlp)\n",
    "    mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "pre_output_concatenate = Concatenate()([gmf_vec,mlp])\n",
    "output = Dense(1, activation='linear', name='output')(pre_output_concatenate)\n",
    "\n",
    "ncf_model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "ncf_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "ncf_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dict = defaultdict(set)\n",
    "\n",
    "# Assuming your train DataFrame has columns 'user_id' and 'movie_id':\n",
    "for user, item in zip(train['user_id'], train['movie_id']):\n",
    "    train_dict[user].add(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train = [train['user_id'].values, train['movie_id'].values]\n",
    "y_train = train['rating_val'].values\n",
    "x_val = [val['user_id'].values, val['movie_id'].values]\n",
    "y_val = val['rating_val'].values\n",
    "x_test = [test['user_id'].values, test['movie_id'].values]\n",
    "y_test = test['rating_val'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_val.shape)\n",
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "# Train the model\n",
    "history = ncf_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1,\n",
    "    callbacks = [early_stop_callback]\n",
    ")\n",
    "#uncomment above to train model again, weights can be loaded aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ncf_model.load_weights('ncf_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ncf_model.save_weights('ncf_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss, mae = ncf_model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "predictions = ncf_model.predict(x_test)\n",
    "rmse = sqrt(np.mean((predictions.flatten() - y_test) ** 2))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_id = 'jay'  # Replace with a valid user_id\n",
    "user_idx = user_mapping.get(user_id)\n",
    "print(user_idx)\n",
    "# Predict ratings for all items\n",
    "all_items = np.arange(n_items)\n",
    "user_tensor = np.array([user_idx] * n_items)\n",
    "predicted_ratings = ncf_model.predict([user_tensor, all_items])\n",
    "\n",
    "# Get top 10 recommendations\n",
    "top_items = np.argsort(predicted_ratings.flatten())[::-1][:10]\n",
    "recommended_movie_ids = [list(item_mapping.keys())[list(item_mapping.values()).index(idx)] for idx in top_items]\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movie_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = test.rename(columns={'rating_val': 'prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# all_user_ids = list(user_mapping.keys())       # e.g. original user IDs\n",
    "# all_user_idxs = list(user_mapping.values())    # integer indices\n",
    "# BATCH_SIZE = 512\n",
    "# all_items = np.arange(n_items)\n",
    "\n",
    "# predictions_list = []\n",
    "# for i in range(0, len(all_user_idxs), BATCH_SIZE):\n",
    "#     # Take a chunk of users\n",
    "#     user_idx_batch = all_user_idxs[i : i + BATCH_SIZE]\n",
    "#     user_id_batch = all_user_ids[i : i + BATCH_SIZE]\n",
    "\n",
    "#     # Repeat items for each user in the batch\n",
    "#     # shape: (#users_in_batch * n_items,)\n",
    "#     tile_users = np.repeat(user_idx_batch, n_items)\n",
    "#     tile_items = np.tile(all_items, len(user_idx_batch))\n",
    "\n",
    "#     # Model predict on that entire chunk\n",
    "#     batch_preds = ncf_model.predict([tile_users, tile_items])  \n",
    "\n",
    "#     # Now we map predictions back to (user, item) pairs\n",
    "#     # We'll build a DataFrame\n",
    "#     df_chunk = pd.DataFrame({\n",
    "#         \"userID\": np.repeat(user_id_batch, n_items),\n",
    "#         \"itemID\": tile_items,\n",
    "#         \"prediction\": batch_preds\n",
    "#     })\n",
    "#     predictions_list.append(df_chunk)\n",
    "\n",
    "# predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "### 1 HOUR 45 MINS FOR ENTIRE DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",\n",
    "        \"user_id\": \"userID\",\n",
    "        \"rating_val\": \"rating\"  \n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "train.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",  \n",
    "        \"user_id\": \"userID\"\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate predictions\n",
    "# #eval_map = map(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "# eval_ndcg = ndcg_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "# print(\"reaches this\")\n",
    "# eval_precision = precision_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "# print(\"reaches this\")\n",
    "# eval_recall = recall_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "# print(\"reaches this\")\n",
    "# print(\n",
    "#     #f\"MAP:\\t{eval_map:.6f}\",\n",
    "#     f\"NDCG:\\t{eval_ndcg:.6f}\",\n",
    "#     f\"Precision@K:\\t{eval_precision:.6f}\",\n",
    "#     f\"Recall@K:\\t{eval_recall:.6f}\",\n",
    "#     sep='\\n'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPLEXITY OF PREDICTING FOR ALL USERS AND ITEMS IS (no_users * no_items) -> VERY LARGE. \n",
    "Use negative sampling - take 1 item user interacted with, and 50 or 100 items the user did NOT interact (did not rate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with manual testing for different users, the model basically outputs the highest rated shows and movies that the user has not rated. how to improve on that ? -> tokenization of movie descriptions into the model, another pipeline with the genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_items_set = set(test['itemID'].unique()) | set(test['itemID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 10\n",
    "hits = []\n",
    "ndcgs = []\n",
    "all_true_labels = []\n",
    "all_pred_scores = []\n",
    "recommended_items = []\n",
    "rating_threshold = 8\n",
    "unique_user_ids = test['userID'].unique()\n",
    "\n",
    "for user_id in tqdm(unique_user_ids[:100], desc=\"Processing users\"):\n",
    "    test_items = test.loc[test['userID'] == user_id & (test['prediction']), 'itemID'].values\n",
    "    pos_items = set(test_items)  \n",
    "    limited_pos_items = list(pos_items)[:3] # take only 3, on processor only takes too slow for more\n",
    "    user_train_items = train_dict[user_id] | set(limited_pos_items)\n",
    "    possible_negatives = list(all_items_set - user_train_items)\n",
    "\n",
    "    for pos_item in limited_pos_items:\n",
    "        neg_sample = random.sample(list(all_items_set - train_dict[user_id] - {pos_item}), 99)\n",
    "        \n",
    "        item_batch = [pos_item] + neg_sample\n",
    "        user_batch = [user_id] * len(item_batch)\n",
    "        \n",
    "        scores = ncf_model.predict([np.array(user_batch), np.array(item_batch)])\n",
    "        scores = np.squeeze(scores)\n",
    "        \n",
    "        labels = np.array([1] + [0]*99) # label the highest rated item as positive \n",
    "        all_true_labels.extend(labels)\n",
    "        all_pred_scores.extend(scores)\n",
    "        \n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        sorted_items = np.array(item_batch)[sorted_indices]\n",
    "        \n",
    "        top_k_items = sorted_items[:k]\n",
    "        recommended_items.extend(top_k_items.tolist())\n",
    "        \n",
    "        if 1 in sorted_labels[:k]:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "        \n",
    "        rank = np.where(sorted_labels[:k] == 1)[0]\n",
    "        if len(rank) > 0:\n",
    "            ndcgs.append(1 / np.log2(rank[0] + 2))  \n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "\n",
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "print(f\"Hit Rate@{k}: {hit_rate:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg:.4f}\")\n",
    "print(f\"ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation :\n",
    "- 45% of users get something relevant to them recommended \n",
    "- ndcg : 29% to an ideal ranking system (higher relevance items raise it )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_pred_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting to check the distribution of the recomended movies for each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "item_counts = Counter(recommended_items)\n",
    "item_counts_df = pd.DataFrame.from_dict(item_counts, orient='index', columns=['count'])\n",
    "item_counts_df.reset_index(inplace=True)\n",
    "item_counts_df.rename(columns={'index': 'itemID'}, inplace=True)\n",
    "item_counts_df.sort_values(by='count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(item_counts_df['count'], bins=50, kde=True)\n",
    "plt.xlabel('Number of Recommendations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Recommended Movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## heavy bias to the highest rated movies (can also be seen by the manual checks for each user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hit@10: 0.4800\n",
    "#NDCG@10: 0.3067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The relevant items is in 48 percent of reccomended lists to the user, ndcg of 30 percent means we are 30 percent to an ideal ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a good baseline, however it can be improved, in multiple ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train GMF and MLP models separately, then combine into new model, fine-tune that one to see if improvements (paper suggests this technique gives better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_gmf_embedding = Embedding(n_users, embedding_dim, name='user_gmf_embedding')(user_input)\n",
    "item_gmf_embedding = Embedding(n_items, embedding_dim, name='item_gmf_embedding')(item_input)\n",
    "\n",
    "# Flatten embeddings\n",
    "user_gmf_vec = Flatten()(user_gmf_embedding)\n",
    "item_gmf_vec = Flatten()(item_gmf_embedding)\n",
    "\n",
    "# Element-wise product of user and item embeddings\n",
    "gmf_vec = Multiply()([user_gmf_vec, item_gmf_vec])\n",
    "\n",
    "# Output layer\n",
    "gmf_output = Dense(1, activation='linear', name='gmf_output')(gmf_vec)\n",
    "\n",
    "# Build and compile GMF model\n",
    "gmf_model = Model(inputs=[user_input, item_input], outputs=gmf_output)\n",
    "gmf_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "gmf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MLP model - neural network \n",
    "user_mlp_embedding = Embedding(n_users, embedding_dim, name='user_mlp_embedding')(user_input)\n",
    "item_mlp_embedding = Embedding(n_items, embedding_dim, name='item_mlp_embedding')(item_input)\n",
    "\n",
    "# Flatten embeddings\n",
    "user_mlp_vec = Flatten()(user_mlp_embedding)\n",
    "item_mlp_vec = Flatten()(item_mlp_embedding)\n",
    "\n",
    "# Concatenate embeddings\n",
    "mlp_vec = Concatenate()([user_mlp_vec, item_mlp_vec])\n",
    "\n",
    "# Fully connected layers\n",
    "mlp = mlp_vec\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp = Dense(size, activation='relu')(mlp)\n",
    "    mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "# Output layer\n",
    "mlp_output = Dense(1, activation='linear', name='mlp_output')(mlp)\n",
    "\n",
    "# Build and compile MLP model\n",
    "mlp_model = Model(inputs=[user_input, item_input], outputs=mlp_output)\n",
    "mlp_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train = [train['user_id'].values, train['movie_id'].values]\n",
    "y_train = train['rating_val'].values\n",
    "x_val = [val['user_id'].values, val['movie_id'].values]\n",
    "y_val = val['rating_val'].values\n",
    "x_test = [test['user_id'].values, test['movie_id'].values]\n",
    "y_test = test['rating_val'].values\n",
    "# Train GMF model\n",
    "gmf_history = gmf_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train MLP model\n",
    "mlp_history = mlp_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#gmf_model.save_weights('ncf_model.weights.h5')\n",
    "gmf_model.save_weights('gmf_model.weights.h5')\n",
    "mlp_model.save_weights('mlp_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  \n",
    "\n",
    "# Weighted combination of GMF and MLP vectors\n",
    "gmf_weighted = Multiply()([gmf_vec, Lambda(lambda x: x * alpha)(gmf_vec)])\n",
    "mlp_weighted = Multiply()([mlp, Lambda(lambda x: x * (1 - alpha))(mlp)])\n",
    "\n",
    "combined = Concatenate()([gmf_weighted, mlp_weighted])\n",
    "\n",
    "final_output = Dense(1, activation='linear', name='final_output')(combined)\n",
    "\n",
    "# Build the final NCF model\n",
    "ncf_combined_model = Model(inputs=[user_input, item_input], outputs=final_output)\n",
    "ncf_combined_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "ncf_combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "alphas = [0.25, 0.5, 0.75]\n",
    "# trained on all 3, balanced one gave best results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gmf_model.load_weights('gmf_model.weights.h5')\n",
    "mlp_model.load_weights('mlp_model.weights.h5')\n",
    "\n",
    "for layer in gmf_model.layers:\n",
    "    if layer.name in ncf_combined_model.layers:\n",
    "        ncf_combined_model.get_layer(layer.name).set_weights(layer.get_weights())\n",
    "\n",
    "for layer in mlp_model.layers:\n",
    "    if layer.name in ncf_combined_model.layers:\n",
    "        ncf_combined_model.get_layer(layer.name).set_weights(layer.get_weights())\n",
    "\n",
    "# Now, you can train the combined model\n",
    "ncf_combined_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 10\n",
    "hits = []\n",
    "ndcgs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "all_true_labels = []\n",
    "all_pred_scores = []\n",
    "recommended_items = []\n",
    "\n",
    "unique_user_ids = test['user_id'].unique()\n",
    "\n",
    "# Evaluation Loop\n",
    "for user_id in tqdm(unique_user_ids[:100], desc=\"Processing users\"):\n",
    "    # Fetch the test items for the user\n",
    "    test_items = test.loc[(test['user_id'] == user_id) & (test['rating_val'] >= rating_threshold), 'movie_id'].values\n",
    "    pos_items = set(test_items)  \n",
    "    limited_pos_items = list(pos_items)[:3] # take only 3, on processor only takes too slow for all \n",
    "    # Build negative set (exclude train_dict[user_id] + the positive items)\n",
    "    user_train_items = train_dict[user_id] | set(limited_pos_items)\n",
    "    possible_negatives = list(all_items_set - user_train_items)\n",
    "\n",
    "    # Iterate over each positive item\n",
    "    for pos_item in limited_pos_items:\n",
    "        # Sample 99 negatives for the current positive item\n",
    "        neg_sample = random.sample(list(all_items_set - train_dict[user_id] - {pos_item}), 99)\n",
    "        \n",
    "        # Combine into a batch\n",
    "        item_batch = [pos_item] + neg_sample\n",
    "        user_batch = [user_id] * len(item_batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        scores = ncf_combined_model.predict([np.array(user_batch), np.array(item_batch)])\n",
    "        scores = np.squeeze(scores)\n",
    "        \n",
    "        # Binary labels: 1 for positive item, 0 for negatives\n",
    "        labels = np.array([1] + [0]*99)\n",
    "        all_true_labels.extend(labels)\n",
    "        all_pred_scores.extend(scores)\n",
    "        \n",
    "        # Sort items by score in descending order\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        sorted_items = np.array(item_batch)[sorted_indices]\n",
    "        \n",
    "        # Collect recommended items for distribution analysis\n",
    "        top_k_items = sorted_items[:k]\n",
    "        recommended_items.extend(top_k_items.tolist())\n",
    "        \n",
    "        # Calculate Hit@K, if positive item in these, its a hit\n",
    "        if 1 in sorted_labels[:k]:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "        \n",
    "        # Calculate NDCG@K\n",
    "        rank = np.where(sorted_labels[:k] == 1)[0]\n",
    "        if len(rank) > 0:\n",
    "            ndcgs.append(1 / np.log2(rank[0] + 2))  \n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "        \n",
    "        # Calculate Precision@K\n",
    "        precision = np.sum(sorted_labels[:k]) / k\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # Calculate Recall@K\n",
    "        recall = np.sum(sorted_labels[:k]) / len(limited_pos_items)\n",
    "        recalls.append(recall)\n",
    "\n",
    "# Calculate Final Metrics\n",
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "precision_at_k = np.mean(precisions)\n",
    "recall_at_k = np.mean(recalls)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(item_counts_df['count'], bins=50, kde=True)\n",
    "plt.xlabel('Number of Recommendations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Recommended Movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "print(f\"Hit Rate@{k}: {hit_rate:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg:.4f}\")\n",
    "print(f\"ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_pred_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('good_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_id = 'jay'  # Replace with a valid user_id\n",
    "user_idx = user_mapping.get(user_id)\n",
    "print(user_idx)\n",
    "# Predict ratings for all items\n",
    "all_items = np.arange(n_items)\n",
    "user_tensor = np.array([user_idx] * n_items)\n",
    "predicted_ratings = ncf_combined_model.predict([user_tensor, all_items])\n",
    "\n",
    "# Get top 10 recommendations\n",
    "top_items = np.argsort(predicted_ratings.flatten())[::-1][:10]\n",
    "recommended_movie_ids = [list(item_mapping.keys())[list(item_mapping.values()).index(idx)] for idx in top_items]\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movie_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP:\t0.044724\n",
    "NDCG:\t0.183073\n",
    "FROM THE RECOMENDERS PAPER I USED "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit Rate@10: 0.6733\n",
    "NDCG@10: 0.4019\n",
    "ROC AUC: 0.8771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ncf_combined_model.save_weights('ncf_combined_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improve on the suggested architecture :\n",
    "- use other information in the dataset: ex. genre, popularity, overview of the movies \n",
    "- add layers to the model - batch normalization, dropout, change loss function\n",
    "- evaluate the model with tenfold cross validation \n",
    "- ensemble with tokenization of movie overview \n",
    "- add precision@k,AUC,recall@k,mean reciprocal rank (MRR)\n",
    "- l1-l2 regularization\n",
    "- autoencoder of user and item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = movie_data_df.dropna(subset=['genres', 'popularity','overview'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turning genres column into one hot encoded columns for each genre. Movies with no genre set should have all zeros in the respective columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_tokens = 20000 # vocab size \n",
    "sequence_length = 300 # output vector length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(movie_data_df['genres'])\n",
    "genre_labels = mlb.classes_  # Get genre labels from the MultiLabelBinarizer\n",
    "print(genre_labels)\n",
    "genre_labels_cleaned = [c.replace(\" \", \"_\") for c in mlb.classes_]\n",
    "\n",
    "df_genres = pd.DataFrame(genres_encoded, columns=genre_labels_cleaned)\n",
    "print(genre_labels_cleaned)\n",
    "# Now the columns will be \"Science_Fiction\" rather than \"Science Fiction\"\n",
    "movie_data_df = pd.concat([movie_data_df, df_genres], axis=1)\n",
    "\n",
    "# Also keep your final list to use in the loop\n",
    "genre_labels = genre_labels_cleaned\n",
    "scaler = MinMaxScaler()\n",
    "movie_data_df['popularity_scaled'] = scaler.fit_transform(movie_data_df[['popularity']])\n",
    "#movie_data_df = pd.concat([movie_data_df, genres_encoded_df], axis=1)\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=sequence_length,\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "sample_size = 50000  # adapt on a sample large enough so that the vectorization keeps the most of the context data as possible in each review\n",
    "sample_overviews = movie_data_df['overview'].dropna().sample(n=sample_size, random_state=42)\n",
    "overview_ds = tf.data.Dataset.from_tensor_slices(sample_overviews)\n",
    "vectorizer.adapt(overview_ds.batch(512))  # adapt in batches for memory efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_overviews = movie_data_df['overview'].fillna(\"\") \n",
    "\n",
    "all_overview_ds = tf.data.Dataset.from_tensor_slices(all_overviews).batch(512)\n",
    "\n",
    "all_overview_ints = []\n",
    "for batch in all_overview_ds:\n",
    "    int_batch = vectorizer(batch)  \n",
    "    all_overview_ints.append(int_batch.numpy())  \n",
    "\n",
    "all_overview_ints = np.concatenate(all_overview_ints, axis=0)\n",
    "\n",
    "movie_data_df['overview_ints'] = list(all_overview_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df['overview_ints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_embedding_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pretrain a model that embeds the texts and pools them \n",
    "\n",
    "\n",
    "text_input = Input(shape=(sequence_length,), dtype='int32', name='text_input')\n",
    "\n",
    "text_emb_layer = Embedding(input_dim=max_tokens, output_dim=text_embedding_dim, name='text_embedding')\n",
    "embedded_seq = text_emb_layer(text_input)  # shape: (batch, sequence_length, text_embedding_dim)\n",
    "\n",
    "overview_embedding = GlobalAveragePooling1D()(embedded_seq)  # shape: (batch, text_embedding_dim)\n",
    "\n",
    "text_model = Model(inputs=text_input, outputs=overview_embedding)\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "overview_ints_dataset = tf.data.Dataset.from_tensor_slices(all_overview_ints).batch(512)\n",
    "\n",
    "all_embeddings = []\n",
    "for batch in overview_ints_dataset:  # first train it on all batches\n",
    "    text_model(batch,training = True)\n",
    "for batch in overview_ints_dataset:\n",
    "    # Pass the batch through text_model\n",
    "    emb_batch = text_model(batch, training=False)  # shape: (batch_size, text_embedding_dim)\n",
    "    all_embeddings.append(emb_batch.numpy())\n",
    "\n",
    "all_embeddings = np.concatenate(all_embeddings, axis=0)  # shape: (num_movies, text_embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df['overview_pre_embed'] = list(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = movie_data_df.drop(['genres'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ratings_df['movie_id'] = ratings_df['movie_id'].astype(int)\n",
    "#movie_data_df['movie_id'] = movie_data_df['movie_id'].astype(int)\n",
    "movie_data_df = movie_data_df.dropna(subset=['movie_id'])\n",
    "ratings_df = ratings_df.dropna(subset=['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(list(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(genre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example function to downcast numerics\n",
    "def downcast_df(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        if str(df[col].dtype) == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        elif str(df[col].dtype) == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df = downcast_df(ratings_df)\n",
    "movie_data_df = downcast_df(movie_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "genre_columns = [col for col in movie_data_df.columns if col in genre_labels]\n",
    "print(genre_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = movie_data_df.drop_duplicates(subset=['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.info(memory_usage='deep'))\n",
    "print(movie_data_df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "useful_cols = ['movie_id', 'overview_pre_embed','popularity_scaled']+ genre_labels #+ genre_columns, add popularity scaled as well \n",
    "movie_data_small = movie_data_df[useful_cols]\n",
    "\n",
    "merged_df_large = ratings_df.merge(\n",
    "    movie_data_small,  \n",
    "    on='movie_id', \n",
    "    how='inner'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(merged_df_large.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df_large = merged_df_large.dropna(subset = genre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train1, val1 = train_test_split(merged_df_large, test_size=0.2, random_state=42)\n",
    "train1, test1 = train_test_split(train1, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_user = train1['user_id'].values\n",
    "X_item = train1['movie_id'].values\n",
    "X_text = np.vstack(train1['overview_pre_embed'].values)  # shape (num_samples, text_embedding_dim)\n",
    "X_popularity = train1['popularity_scaled'].values.reshape(-1, 1)  # shape (num_samples, 1)\n",
    "X_genres = train1[genre_labels].values  # shape (num_samples, len(genre_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = train1['rating_val'].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(val1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_users = X_user.max() + 1\n",
    "num_items = X_item.max() + 1\n",
    "num_genres = len(genre_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Data shapes:\")\n",
    "print(\"X_user:\", X_user.shape)\n",
    "print(\"X_item:\", X_item.shape)\n",
    "print(\"X_text:\", X_text.shape)\n",
    "print(\"X_popularity:\", X_popularity.shape)\n",
    "print(\"X_genres:\", X_genres.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "mlp_layer_sizes = [64, 32, 16]\n",
    "n_users = 10000\n",
    "n_items = 5000\n",
    "num_genres = 19\n",
    "overview_dim = 32\n",
    "\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "genre_input = Input(shape=(num_genres,), name='genre_input')\n",
    "popularity_input = Input(shape=(1,), name='popularity_input')\n",
    "overview_input = Input(shape=(overview_dim,), name='overview_input')\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Embeddings (Reused)\n",
    "# ---------------------------\n",
    "user_embedding = Embedding(n_users, embedding_dim, name='user_embedding')\n",
    "item_embedding = Embedding(n_items, embedding_dim, name='item_embedding')\n",
    "\n",
    "user_vec = Flatten()(user_embedding(user_input))\n",
    "item_vec = Flatten()(item_embedding(item_input))\n",
    "\n",
    "# ---------------------------\n",
    "# 3) GMF path (element-wise product)\n",
    "# ---------------------------\n",
    "gmf_vec = Multiply(name='gmf_interaction')([user_vec, item_vec])\n",
    "\n",
    "# ---------------------------\n",
    "# 4) MLP path (concat user_vec, item_vec & item features)\n",
    "# ---------------------------\n",
    "# Reduce overview dimension\n",
    "#overview_reduced = Dense(32, activation='relu', name='overview_reduced')(overview_input)\n",
    "\n",
    "item_features = Concatenate(name='item_features_concat')([\n",
    "    item_vec,\n",
    "    genre_input,\n",
    "    popularity_input,\n",
    "    overview_input\n",
    "])\n",
    "\n",
    "mlp_input = Concatenate(name='mlp_concat')([\n",
    "    user_vec,\n",
    "    item_features\n",
    "])\n",
    "\n",
    "mlp = mlp_input\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp = Dense(size, activation='relu')(mlp)\n",
    "    mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "mlp_vec = mlp\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Final: Concatenate GMF & MLP + Output\n",
    "# ---------------------------\n",
    "final_concat = Concatenate(name='final_concat')([gmf_vec, mlp_vec])\n",
    "output = Dense(1, activation='linear', name='output')(final_concat)\n",
    "\n",
    "ncf_model = Model(\n",
    "    inputs=[\n",
    "        user_input,\n",
    "        item_input,\n",
    "        genre_input,\n",
    "        popularity_input,\n",
    "        overview_input\n",
    "    ],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Compile & Summary\n",
    "# ---------------------------\n",
    "ncf_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "ncf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    X_user,        # user_input\n",
    "    X_item,        # item_input\n",
    "    X_text,        # text_input\n",
    "    X_popularity,  # popularity_input\n",
    "    X_genres       # genre_input\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     x= X_train,\n",
    "#     y=y,\n",
    "#     batch_size=256,\n",
    "#     epochs=5,\n",
    "#     #validation_split=0.1\n",
    "#     validation_data = [val1]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1) user_id: shape (num_samples,)\n",
    "user_ids_train = train1['user_id'].values  \n",
    "\n",
    "# 2) movie_id: shape (num_samples,)\n",
    "item_ids_train = train1['movie_id'].values\n",
    "\n",
    "# 3) genre vector: shape (num_samples, num_genres)\n",
    "genre_train = train1[genre_labels].values  \n",
    "\n",
    "# 4) popularity: shape (num_samples, 1)\n",
    "popularity_train = train1['popularity_scaled'].values.reshape(-1, 1)\n",
    "\n",
    "# 5) overview: shape (num_samples, overview_dim)\n",
    "overview_train = np.vstack(train1['overview_pre_embed'].values)\n",
    "\n",
    "# y: shape (num_samples,)\n",
    "y_train = train1['rating_val'].values\n",
    "\n",
    "x_train = [\n",
    "    user_ids_train,       \n",
    "    item_ids_train,       \n",
    "    genre_train,          \n",
    "    popularity_train,     \n",
    "    overview_train        \n",
    "]\n",
    "\n",
    "user_ids_val = val1['user_id'].values\n",
    "item_ids_val = val1['movie_id'].values\n",
    "genre_val = val1[genre_labels].values\n",
    "popularity_val = val1['popularity_scaled'].values.reshape(-1, 1)\n",
    "overview_val = np.vstack(val1['overview_pre_embed'].values)\n",
    "y_val = val1['rating_val'].values\n",
    "\n",
    "x_val = [\n",
    "    user_ids_val,\n",
    "    item_ids_val,\n",
    "    genre_val,\n",
    "    popularity_val,\n",
    "    overview_val\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.isnan(y_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.isnan(overview_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.isnan(popularity_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.isnan(genre_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#big_model.load_weights('ncf_combined_model.weights.h5')\n",
    "# try without loading the pretrained first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = ncf_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ncf_model.load_weights('ncf_model_big.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ncf_model.save_weights('ncf_model_big.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dict1 = defaultdict(set)\n",
    "\n",
    "# Assuming your train DataFrame has columns 'user_id' and 'movie_id':\n",
    "for user, item in zip(train1['user_id'], train1['movie_id']):\n",
    "    train_dict1[user].add(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_items_set = set(train1['movie_id'].unique()) | set(test1['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "item_overview_map = dict(zip(movie_data_df['movie_id'], movie_data_df['overview_pre_embed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "item_genre_map = {}\n",
    "item_popularity_map = {}\n",
    "item_overview_map = {}\n",
    "test_item_ids = set(test1['movie_id'].unique())\n",
    "\n",
    "sub_df = merged_df_large[merged_df_large['movie_id'].isin(test_item_ids)]\n",
    "\n",
    "for idx, row in tqdm(sub_df.iterrows(), \n",
    "                     total=len(sub_df), \n",
    "                     desc=\"Processing test items\"):\n",
    "    item_id = row[\"movie_id\"]\n",
    "    genre_vec = np.array([row[g] for g in genre_labels], dtype=np.float32)\n",
    "\n",
    "    # Suppose you have genre columns g1...g19 in your DF\n",
    "    # Create the genre vector:\n",
    "    # If you stored them as separate columns, collect them:\n",
    "    \n",
    "    # Popularity\n",
    "    popularity_val = row.popularity_scaled  # or getattr(row, 'popularity_scaled')\n",
    "\n",
    "    # Overview pre-embedding (if stored as a list in the DF, you might do something else)\n",
    "    # If you stored them as an actual Python list or array in the DataFrame, just reference it\n",
    "    overview_vec = np.array(row.overview_pre_embed, dtype=np.float32)\n",
    "\n",
    "    item_genre_map[item_id] = genre_vec\n",
    "    item_popularity_map[item_id] = popularity_val\n",
    "    item_overview_map[item_id] = overview_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test1[genre_labels].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(movie_data_df[genre_labels].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "k = 10\n",
    "hits = []\n",
    "ndcgs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "all_true_labels = []\n",
    "all_pred_scores = []\n",
    "recommended_items = []\n",
    "\n",
    "unique_user_ids = test1['user_id'].unique()\n",
    "\n",
    "all_items_set = set(movie_data_df['movie_id'])  \n",
    "\n",
    "for user_id in tqdm(unique_user_ids[:100], desc=\"Processing users\"):\n",
    "    # 1) Fetch the test (positive) items for this user - he rated (interacted)\n",
    "    test_items = test1.loc[test1['user_id'] == user_id, 'movie_id'].values\n",
    "    pos_items = set(test_items)\n",
    "\n",
    "    # If you only want to evaluate, say, 3 positives:\n",
    "    limited_pos_items = list(pos_items)[:3]\n",
    "\n",
    "    # 2) Retrieve training items for user (to exclude them in negative sampling)\n",
    "    #    Assuming you have a dict: train_dict[user_id] = set of items user has interacted with in train\n",
    "    user_train_items = train_dict1[user_id] | set(limited_pos_items)\n",
    "    \n",
    "    # 3) Negative sampling\n",
    "    possible_negatives = all_items_set - user_train_items\n",
    "\n",
    "    for pos_item in limited_pos_items:\n",
    "        # Sample 99 negatives\n",
    "        neg_sample = random.sample(list(possible_negatives - {pos_item}), 99)\n",
    "\n",
    "        # item_batch are the actual item IDs\n",
    "        item_batch = [pos_item] + neg_sample\n",
    "\n",
    "        # user_batch is the user repeated\n",
    "        user_batch = [user_id] * len(item_batch)\n",
    "        item_batch_np = np.array(item_batch, dtype=np.int32)\n",
    "        item_batch = [int(i) for i in item_batch]\n",
    "        # 2) genre_batch_np\n",
    "        genre_batch = [item_genre_map[i] for i in item_batch]\n",
    "        genre_batch_np = np.array(genre_batch, dtype=np.float32)\n",
    "        \n",
    "        pop_batch = [item_popularity_map[i] for i in item_batch]\n",
    "        popularity_batch_np = np.array(pop_batch, dtype=np.float32).reshape(-1, 1)\n",
    "        \n",
    "        # 4) overview_batch_np\n",
    "        ov_batch = [item_overview_map[i] for i in item_batch]\n",
    "        overview_batch_np = np.array(ov_batch, dtype=np.float32)\n",
    "        # 4) Build item_emb_batch by looking up each item ID in the embedding map\n",
    "        #item_emb_batch = [item_overview_map[item] for item in item_batch]\n",
    "        # item_emb_batch is now a list of arrays (each shape = (text_embedding_dim,))\n",
    "\n",
    "        # Convert to NumPy\n",
    "        user_batch_np = np.array(user_batch, dtype=np.int32)         # shape: (batch_size,)\n",
    "        #item_emb_batch_np = np.array(item_emb_batch, dtype=np.float32)  # shape: (batch_size, text_embedding_dim)\n",
    "\n",
    "        # 5) Predict\n",
    "        scores = ncf_model.predict([\n",
    "        user_batch_np,\n",
    "        item_batch_np,\n",
    "        genre_batch_np,\n",
    "        popularity_batch_np,\n",
    "        overview_batch_np\n",
    "        ], verbose=0)\n",
    "        scores = np.squeeze(scores) \n",
    "\n",
    "        # 6) Prepare labels (1 positive, 99 negatives)\n",
    "        labels = np.array([1] + [0]*99)\n",
    "        all_true_labels.extend(labels)\n",
    "        all_pred_scores.extend(scores)\n",
    "\n",
    "        # 7) Sort items by predicted score (descending)\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        sorted_items = np.array(item_batch)[sorted_indices]\n",
    "\n",
    "        # 8) Evaluate metrics\n",
    "        # Hit@K\n",
    "        if 1 in sorted_labels[:k]:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "        # NDCG@K\n",
    "        rank = np.where(sorted_labels[:k] == 1)[0]\n",
    "        if len(rank) > 0:\n",
    "            ndcgs.append(1 / np.log2(rank[0] + 2))  \n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "\n",
    "        # Precision@K\n",
    "        precision = np.sum(sorted_labels[:k]) / k\n",
    "        precisions.append(precision)\n",
    "\n",
    "        # Recall@K\n",
    "        recall = np.sum(sorted_labels[:k]) / len(limited_pos_items)\n",
    "        recalls.append(recall)\n",
    "\n",
    "# 9) Compute final metrics\n",
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "precision_at_k = np.mean(precisions)\n",
    "recall_at_k = np.mean(recalls)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "print(f\"Hit@{k}: {hit_rate}\")\n",
    "print(f\"NDCG@{k}: {ndcg}\")\n",
    "print(f\"Precision@{k}: {precision_at_k}\")\n",
    "print(f\"Recall@{k}: {recall_at_k}\")\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Any NaN in user_batch_np?\", np.isnan(user_batch_np).any())\n",
    "print(\"Any NaN in item_batch_np?\", np.isnan(item_batch_np).any())\n",
    "print(\"Any NaN in genre_batch_np?\", np.isnan(genre_batch_np).any())\n",
    "print(\"Any NaN in popularity_batch_np?\", np.isnan(popularity_batch_np).any())\n",
    "print(\"Any NaN in overview_batch_np?\", np.isnan(overview_batch_np).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_pred_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pretrained GMF,MLP and overview based model combined together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_embedding_dim = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "text_input = Input(shape=(sequence_length,), dtype='int32', name='text_input')\n",
    "\n",
    "# ---------------------------\n",
    "# 2) User Embedding\n",
    "# ---------------------------\n",
    "user_embedding_layer = Embedding(\n",
    "    input_dim=n_users, \n",
    "    output_dim=user_embedding_dim, \n",
    "    name='user_embedding'\n",
    ")\n",
    "user_emb = user_embedding_layer(user_input)  # shape (None, 1, user_embedding_dim)\n",
    "user_vec = Flatten()(user_emb)               # shape (None, user_embedding_dim)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Text Embedding\n",
    "# ---------------------------\n",
    "text_embedding_layer = Embedding(\n",
    "    input_dim=max_tokens, \n",
    "    output_dim=text_embedding_dim, \n",
    "    name='text_embedding'\n",
    ")\n",
    "text_embedded_seq = text_embedding_layer(text_input)  \n",
    "# shape (None, sequence_length, text_embedding_dim)\n",
    "\n",
    "# Global average pooling to get a single vector\n",
    "text_vec = GlobalAveragePooling1D()(text_embedded_seq)\n",
    "# shape (None, text_embedding_dim)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Combine User + Text\n",
    "# ---------------------------\n",
    "# Simple approach: concatenate user vector & text vector, then MLP\n",
    "combined_vec = Concatenate()([user_vec, text_vec])  # shape (None, user_embedding_dim + text_embedding_dim)\n",
    "\n",
    "x = Dense(64, activation='relu')(combined_vec)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Final rating output\n",
    "output = Dense(1, activation='linear', name='rating_output')(x)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Build & Compile\n",
    "# ---------------------------\n",
    "text_only_cf_model = Model(\n",
    "    inputs=[user_input, text_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "text_only_cf_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "text_only_cf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "useful_cols = ['movie_id', 'overview']\n",
    "movie_data_small = movie_data_df[useful_cols]\n",
    "merged_df = ratings_df.merge(\n",
    "    movie_data_small,  \n",
    "    on='movie_id', \n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_size = 150000\n",
    "df_sample = merged_df.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_tokens, \n",
    "                      oov_token=\"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "overview_list = list(tqdm(df_sample['overview'], desc=\"Collecting sample texts\"))\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer.fit_on_texts(overview_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_overview_sequences = tokenizer.texts_to_sequences(tqdm(df_sample['overview'], \n",
    "                                                          desc=\"Converting all texts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_input_data = pad_sequences(all_overview_sequences, \n",
    "                                maxlen=sequence_length, \n",
    "                                padding='post', \n",
    "                                truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(text_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_users = df_sample['user_id'].unique()\n",
    "user2idx = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
    "df_sample['user_id_mapped'] = df_sample['user_id'].map(user2idx)\n",
    "n_users = len(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings = df_sample['rating_val'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train2, val2 = train_test_split(df_sample, test_size=0.2, random_state=42)\n",
    "train2, test2 = train_test_split(train2, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_overview_sequences2 = tokenizer.texts_to_sequences(tqdm(train2['overview'], \n",
    "                                                          desc=\"Converting all train texts\"))\n",
    "text_input_data = pad_sequences(all_overview_sequences2, \n",
    "                                maxlen=sequence_length, \n",
    "                                padding='post', \n",
    "                                truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_overview_sequences2_val = tokenizer.texts_to_sequences(tqdm(val2['overview'], \n",
    "                                                          desc=\"Converting all train texts\"))\n",
    "text_input_data_val = pad_sequences(all_overview_sequences2_val, \n",
    "                                maxlen=sequence_length, \n",
    "                                padding='post', \n",
    "                                truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_overview_sequences2_test = tokenizer.texts_to_sequences(tqdm(test2['overview'], \n",
    "                                                          desc=\"Converting all train texts\"))\n",
    "text_input_data_test = pad_sequences(all_overview_sequences2_val, \n",
    "                                maxlen=sequence_length, \n",
    "                                padding='post', \n",
    "                                truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train2['overview_length'] = train2['overview'].apply(len)\n",
    "\n",
    "avg_length = train2['overview_length'].mean()\n",
    "min_length = train2['overview_length'].min()\n",
    "max_length = train2['overview_length'].max()\n",
    "train2.drop(['overview_length'],axis = 1)\n",
    "print(f\"Average Overview Length: {avg_length:.2f}\")\n",
    "print(f\"Minimum Overview Length: {min_length}\")\n",
    "print(f\"Maximum Overview Length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_embedding_dim = 32\n",
    "sequence_length = 32 # 1000 with untokenized \n",
    "text_input = Input(shape=(sequence_length,), dtype='int32', name='text_input')\n",
    "\n",
    "text_emb_layer = Embedding(input_dim=max_tokens, output_dim=text_embedding_dim, name='text_embedding')\n",
    "embedded_seq = text_emb_layer(text_input)  # (batch, sequence_length, text_embedding_dim)\n",
    "\n",
    "# Global average pooling to get fixed-size vector (32)\n",
    "text_embedding = GlobalAveragePooling1D()(embedded_seq)  # (batch, 32)\n",
    "\n",
    "# Define text embedding model\n",
    "text_model = Model(inputs=text_input, outputs=text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_user_input = train2['user_id_mapped'].values\n",
    "train_texts = train2['overview'].values\n",
    "train_y = train2['rating_val'].values\n",
    "\n",
    "# Validation sets\n",
    "val_user_input = val2['user_id_mapped'].values\n",
    "val_texts = train2['overview'].values\n",
    "val_y = val2['rating_val'].values\n",
    "\n",
    "# Test sets\n",
    "test_user_input = test2['user_id_mapped'].values\n",
    "test_texts = train2['overview'].values\n",
    "test_y = test2['rating_val'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), name='user_input')\n",
    "\n",
    "# User embedding layer (same as before)\n",
    "user_embedding = Embedding(n_users, embedding_dim, name='user_embedding')(user_input)\n",
    "user_vec = Flatten()(user_embedding)  # (batch, 32)\n",
    "\n",
    "# Get text embedding from the pre-trained text model\n",
    "item_embedding = text_model(text_input)  # (batch, 32)\n",
    "text_embedding_1000 = text_model(text_input)            # shape: (batch, 1000)\n",
    "item_embedding = Dense(32, activation='relu')(\n",
    "    text_embedding_1000\n",
    ")  \n",
    "# GMF component\n",
    "gmf_vec = Multiply()([user_vec, item_embedding])\n",
    "\n",
    "# MLP component\n",
    "concat_vec = Concatenate()([user_vec, item_embedding])  # (batch, 64)\n",
    "mlp = concat_vec\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp = Dense(size, activation='relu')(mlp)\n",
    "    mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "# Final prediction layer\n",
    "pre_output_concatenate = Concatenate()([gmf_vec, mlp])  # (batch, 48)\n",
    "output = Dense(1, activation='linear', name='output')(pre_output_concatenate)\n",
    "\n",
    "# Build the full NCF model\n",
    "ncf_text_model = Model(inputs=[user_input, text_input], outputs=output)\n",
    "ncf_text_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "ncf_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ncf_text_model.fit([train_user_input, text_input_data], train_y, validation_data=([val_user_input, text_input_data_val], val_y), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dict = defaultdict(set)\n",
    "\n",
    "# Assuming your train DataFrame has columns 'user_id' and 'movie_id':\n",
    "for user, item in zip(train2['user_id'], train2['movie_id']):\n",
    "    train_dict[user].add(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_items_set = set(train2['movie_id'].unique()) | set(test2['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "item2index_all = {}\n",
    "for i, row in df_sample.iterrows():\n",
    "    item_id = row['movie_id']\n",
    "    item2index[item_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test2 = test2.reset_index(drop=True)   # Now test2.index runs 0..29999\n",
    "item2index = {\n",
    "    test2.loc[i, 'movie_id']: i\n",
    "    for i in range(len(test2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "k = 10\n",
    "hits = []\n",
    "ndcgs = []\n",
    "all_true_labels = []\n",
    "all_pred_scores = []\n",
    "recommended_items = []\n",
    "rating_threshold = 8\n",
    "unique_user_ids = test2['user_id'].unique()\n",
    "\n",
    "for user_id in tqdm(unique_user_ids[:100], desc=\"Processing users\"):\n",
    "    # Fetch positive items from the test set\n",
    "    test_items = test2.loc[\n",
    "        (test2['user_id'] == user_id) & (test2['rating_val'] >= rating_threshold),\n",
    "        'movie_id'\n",
    "    ].values\n",
    "    pos_items = set(test_items)\n",
    "    limited_pos_items = list(pos_items)[:3]\n",
    "\n",
    "    user_train_items = train_dict[user_id] | set(limited_pos_items)\n",
    "    possible_negatives = list(all_items_set - user_train_items)\n",
    "\n",
    "    # Iterate over each positive item\n",
    "    for pos_item in limited_pos_items:\n",
    "        # Sample 99 negatives\n",
    "        neg_sample = random.sample(\n",
    "            list(all_items_set - train_dict[user_id] - {pos_item}),\n",
    "            99\n",
    "        )\n",
    "        \n",
    "        # Combine into a batch\n",
    "        item_batch = [pos_item] + neg_sample  # length=100\n",
    "        mapped_indices = [item2index_all[item_id] for item_id in item_batch]\n",
    "        item_batch_texts = text_input_data_test[mapped_indices]  \n",
    "        user_batch = [user_id] * len(item_batch)\n",
    "\n",
    "        # Reshape user batch if needed:\n",
    "        user_batch_array = np.array(user_batch).reshape(-1, 1)\n",
    "\n",
    "        # Get text data for items\n",
    "        #item_batch_texts = text_input_data_test[item_batch]  # shape: (100, sequence_length)\n",
    "        \n",
    "        # Get predictions from your text-based NCF model\n",
    "        scores = ncf_text_model.predict([user_batch_array, item_batch_texts])\n",
    "        scores = np.squeeze(scores)  # (100,)\n",
    "\n",
    "        # Prepare binary labels (1 for pos_item, 0 for negative items)\n",
    "        labels = np.array([1] + [0]*99)\n",
    "        all_true_labels.extend(labels)\n",
    "        all_pred_scores.extend(scores)\n",
    "\n",
    "        # Sort items by score (descending)\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        sorted_items = np.array(item_batch)[sorted_indices]\n",
    "\n",
    "        # Grab top-K\n",
    "        top_k_items = sorted_items[:k]\n",
    "        recommended_items.extend(top_k_items.tolist())\n",
    "\n",
    "        # HIT@K\n",
    "        if 1 in sorted_labels[:k]:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "        # NDCG@K\n",
    "        rank = np.where(sorted_labels[:k] == 1)[0]\n",
    "        if len(rank) > 0:\n",
    "            ndcgs.append(1 / np.log2(rank[0] + 2))\n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "\n",
    "# Compute final metrics\n",
    "hit_rate = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "print(f\"Hit@{k}: {hit_rate}, NDCG@{k}: {ndcg}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1300195,
     "sourceId": 3341890,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6442594,
     "sourceId": 10397826,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 233959,
     "modelInstanceId": 212291,
     "sourceId": 248380,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 233965,
     "modelInstanceId": 212298,
     "sourceId": 248387,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
