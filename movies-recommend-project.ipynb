{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:50.396864Z",
     "iopub.status.busy": "2025-01-26T10:58:50.396523Z",
     "iopub.status.idle": "2025-01-26T10:58:54.207888Z",
     "shell.execute_reply": "2025-01-26T10:58:54.206936Z",
     "shell.execute_reply.started": "2025-01-26T10:58:50.396834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install recommenders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.209616Z",
     "iopub.status.busy": "2025-01-26T10:58:54.209385Z",
     "iopub.status.idle": "2025-01-26T10:58:54.215831Z",
     "shell.execute_reply": "2025-01-26T10:58:54.214976Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.209597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout,Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from math import sqrt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from time import time\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "import random\n",
    "import json\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 : LOADING THE DATASET FROM KAGGLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.217749Z",
     "iopub.status.busy": "2025-01-26T10:58:54.217517Z",
     "iopub.status.idle": "2025-01-26T10:58:54.235591Z",
     "shell.execute_reply": "2025-01-26T10:58:54.234773Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.217724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "devices = tf.config.get_visible_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.237184Z",
     "iopub.status.busy": "2025-01-26T10:58:54.236888Z",
     "iopub.status.idle": "2025-01-26T10:58:54.251339Z",
     "shell.execute_reply": "2025-01-26T10:58:54.250476Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.237155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.252461Z",
     "iopub.status.busy": "2025-01-26T10:58:54.252207Z",
     "iopub.status.idle": "2025-01-26T10:58:54.347205Z",
     "shell.execute_reply": "2025-01-26T10:58:54.346580Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.252435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"samlearner/letterboxd-movie-ratings-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.348205Z",
     "iopub.status.busy": "2025-01-26T10:58:54.347935Z",
     "iopub.status.idle": "2025-01-26T10:58:54.352204Z",
     "shell.execute_reply": "2025-01-26T10:58:54.351445Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.348178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_filename = 'movie_data.csv'\n",
    "ratings_filename = 'ratings_export.csv'\n",
    "users_filename = 'users_export.csv'\n",
    "movie_data_path = os.path.join(path,movie_data_filename)\n",
    "ratings_path = os.path.join(path,ratings_filename)\n",
    "users_path = os.path.join(path,users_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.353329Z",
     "iopub.status.busy": "2025-01-26T10:58:54.353064Z",
     "iopub.status.idle": "2025-01-26T10:58:54.366258Z",
     "shell.execute_reply": "2025-01-26T10:58:54.365583Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.353302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# movie_data_df = pd.read_csv(movie_data_path,engine='python')\n",
    "# ratings_df = pd.read_csv(ratings_path,engine='python')\n",
    "# users_df = pd.read_csv(users_path,engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:58:54.368930Z",
     "iopub.status.busy": "2025-01-26T10:58:54.368722Z",
     "iopub.status.idle": "2025-01-26T10:59:02.638432Z",
     "shell.execute_reply": "2025-01-26T10:59:02.637759Z",
     "shell.execute_reply.started": "2025-01-26T10:58:54.368912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_parquet('movie_data.parquet', engine='pyarrow')\n",
    "movie_data_df = pd.read_parquet('/kaggle/input/parquet-dataset/movie_data.parquet',engine='pyarrow')\n",
    "ratings_df = pd.read_parquet('/kaggle/input/parquet-dataset/ratings_data.parquet',engine='pyarrow')\n",
    "users_df = pd.read_parquet('/kaggle/input/parquet-dataset/users_dt.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:02.640169Z",
     "iopub.status.busy": "2025-01-26T10:59:02.639880Z",
     "iopub.status.idle": "2025-01-26T10:59:02.651418Z",
     "shell.execute_reply": "2025-01-26T10:59:02.650781Z",
     "shell.execute_reply.started": "2025-01-26T10:59:02.640148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unneccessary columns need to be removed - image url, imdb id, imdb, link (perhaps needed to enhance model with imdb data ? ), tmbd id, link, (download that dataset and enhance with it ? )production countries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:02.652745Z",
     "iopub.status.busy": "2025-01-26T10:59:02.652389Z",
     "iopub.status.idle": "2025-01-26T10:59:02.863282Z",
     "shell.execute_reply": "2025-01-26T10:59:02.862524Z",
     "shell.execute_reply.started": "2025-01-26T10:59:02.652712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(movie_data_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:02.864343Z",
     "iopub.status.busy": "2025-01-26T10:59:02.864045Z",
     "iopub.status.idle": "2025-01-26T10:59:02.870267Z",
     "shell.execute_reply": "2025-01-26T10:59:02.869297Z",
     "shell.execute_reply.started": "2025-01-26T10:59:02.864312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:02.871567Z",
     "iopub.status.busy": "2025-01-26T10:59:02.871092Z",
     "iopub.status.idle": "2025-01-26T10:59:02.886994Z",
     "shell.execute_reply": "2025-01-26T10:59:02.886241Z",
     "shell.execute_reply.started": "2025-01-26T10:59:02.871545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:02.887906Z",
     "iopub.status.busy": "2025-01-26T10:59:02.887659Z",
     "iopub.status.idle": "2025-01-26T10:59:03.505956Z",
     "shell.execute_reply": "2025-01-26T10:59:03.505229Z",
     "shell.execute_reply.started": "2025-01-26T10:59:02.887886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in movie_data_df.select_dtypes(include='object'):\n",
    "    print(f\"{col}: {movie_data_df[col].nunique()} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:03.506953Z",
     "iopub.status.busy": "2025-01-26T10:59:03.506743Z",
     "iopub.status.idle": "2025-01-26T10:59:03.510140Z",
     "shell.execute_reply": "2025-01-26T10:59:03.509368Z",
     "shell.execute_reply.started": "2025-01-26T10:59:03.506934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#movie_data_df.to_parquet('movie_data.parquet', engine='pyarrow')\n",
    "#ratings_df.to_parquet('ratings_data.parquet', engine = 'pyarrow')\n",
    "#users_df.to_parquet('users_dt.parquet',engine = 'pyarrow')\n",
    "# Load from Parquet file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: EXPLORATORY DATA ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:03.511261Z",
     "iopub.status.busy": "2025-01-26T10:59:03.510998Z",
     "iopub.status.idle": "2025-01-26T10:59:03.626607Z",
     "shell.execute_reply": "2025-01-26T10:59:03.625726Z",
     "shell.execute_reply.started": "2025-01-26T10:59:03.511240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df = movie_data_df.dropna(subset=['release_date'])\n",
    "print(movie_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:03.627858Z",
     "iopub.status.busy": "2025-01-26T10:59:03.627537Z",
     "iopub.status.idle": "2025-01-26T10:59:03.919761Z",
     "shell.execute_reply": "2025-01-26T10:59:03.918842Z",
     "shell.execute_reply.started": "2025-01-26T10:59:03.627827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "movie_data_df['release_date'] = pd.to_datetime(movie_data_df['release_date'], format='%Y-%m-%d')\n",
    "\n",
    "# Now, sorting by release_date will sort chronologically\n",
    "movie_data_df.sort_values('release_date', inplace=True)\n",
    "print(movie_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:03.920719Z",
     "iopub.status.busy": "2025-01-26T10:59:03.920468Z",
     "iopub.status.idle": "2025-01-26T10:59:04.486057Z",
     "shell.execute_reply": "2025-01-26T10:59:04.485374Z",
     "shell.execute_reply.started": "2025-01-26T10:59:03.920699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df['_id'] = ratings_df['_id'].astype(str)\n",
    "movie_data_df['_id'] = movie_data_df['_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:04.487133Z",
     "iopub.status.busy": "2025-01-26T10:59:04.486914Z",
     "iopub.status.idle": "2025-01-26T10:59:15.035179Z",
     "shell.execute_reply": "2025-01-26T10:59:15.034481Z",
     "shell.execute_reply.started": "2025-01-26T10:59:04.487114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df = ratings_df.merge(\n",
    "    movie_data_df[['movie_id', 'release_date']],  \n",
    "    on='movie_id', \n",
    "    how='inner'  # or 'inner' if you only want ratings for movies that exist in movie_data_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:15.036342Z",
     "iopub.status.busy": "2025-01-26T10:59:15.036126Z",
     "iopub.status.idle": "2025-01-26T10:59:15.719486Z",
     "shell.execute_reply": "2025-01-26T10:59:15.718531Z",
     "shell.execute_reply.started": "2025-01-26T10:59:15.036323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'release_date': 'timestamp'}, inplace=True)\n",
    "merged_df = merged_df.drop(['_id'],axis = 1)\n",
    "# Now, merged_df contains all columns from ratings_df plus the 'timestamp' column\n",
    "# so that python can chrono split the data \n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:15.720829Z",
     "iopub.status.busy": "2025-01-26T10:59:15.720484Z",
     "iopub.status.idle": "2025-01-26T10:59:15.983137Z",
     "shell.execute_reply": "2025-01-26T10:59:15.982447Z",
     "shell.execute_reply.started": "2025-01-26T10:59:15.720793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",\n",
    "        \"rating_val\": \"rating\",\n",
    "        \"user_id\": \"userID\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:15.984167Z",
     "iopub.status.busy": "2025-01-26T10:59:15.983939Z",
     "iopub.status.idle": "2025-01-26T10:59:25.488175Z",
     "shell.execute_reply": "2025-01-26T10:59:25.486988Z",
     "shell.execute_reply.started": "2025-01-26T10:59:15.984147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(['userID', 'timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: DEFINING THE TASK, EVALUATION METRICS, CONSTRUCTING MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:59:25.489333Z",
     "iopub.status.busy": "2025-01-26T10:59:25.489091Z",
     "iopub.status.idle": "2025-01-26T10:59:28.762605Z",
     "shell.execute_reply": "2025-01-26T10:59:28.758620Z",
     "shell.execute_reply.started": "2025-01-26T10:59:25.489300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T10:59:28.763061Z",
     "iopub.status.idle": "2025-01-26T10:59:28.763338Z",
     "shell.execute_reply": "2025-01-26T10:59:28.763225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('./checkpoints/ncf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T10:59:28.764143Z",
     "iopub.status.idle": "2025-01-26T10:59:28.764487Z",
     "shell.execute_reply": "2025-01-26T10:59:28.764321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# texts = movie_data_df['overview'].fillna('').tolist()\n",
    "# inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\", max_length=128)\n",
    "# with strategy.scope():\n",
    "#     outputs = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# # Get embeddings (use outputs.last_hidden_state or outputs.pooler_output)\n",
    "# movie_df['embedding'] = embeddings.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:05.609264Z",
     "iopub.status.busy": "2025-01-26T11:00:05.608959Z",
     "iopub.status.idle": "2025-01-26T11:00:07.678019Z",
     "shell.execute_reply": "2025-01-26T11:00:07.677332Z",
     "shell.execute_reply.started": "2025-01-26T11:00:05.609237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df.dropna(subset=['user_id', 'movie_id', 'rating_val'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:07.679595Z",
     "iopub.status.busy": "2025-01-26T11:00:07.679168Z",
     "iopub.status.idle": "2025-01-26T11:00:07.688113Z",
     "shell.execute_reply": "2025-01-26T11:00:07.687365Z",
     "shell.execute_reply.started": "2025-01-26T11:00:07.679560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:07.689619Z",
     "iopub.status.busy": "2025-01-26T11:00:07.689414Z",
     "iopub.status.idle": "2025-01-26T11:00:07.702856Z",
     "shell.execute_reply": "2025-01-26T11:00:07.702124Z",
     "shell.execute_reply.started": "2025-01-26T11:00:07.689600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non integer frames must be mapped to a unique numeric value - movie id and user id in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:07.704415Z",
     "iopub.status.busy": "2025-01-26T11:00:07.704142Z",
     "iopub.status.idle": "2025-01-26T11:00:09.411837Z",
     "shell.execute_reply": "2025-01-26T11:00:09.410937Z",
     "shell.execute_reply.started": "2025-01-26T11:00:07.704395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_mapping = {user: idx for idx, user in enumerate(ratings_df['user_id'].unique())}\n",
    "item_mapping = {item: idx for idx, item in enumerate(ratings_df['movie_id'].unique())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:09.413034Z",
     "iopub.status.busy": "2025-01-26T11:00:09.412735Z",
     "iopub.status.idle": "2025-01-26T11:00:09.418812Z",
     "shell.execute_reply": "2025-01-26T11:00:09.418082Z",
     "shell.execute_reply.started": "2025-01-26T11:00:09.413000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_items = list(user_mapping.items())[:10]\n",
    "print(sample_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:09.419865Z",
     "iopub.status.busy": "2025-01-26T11:00:09.419584Z",
     "iopub.status.idle": "2025-01-26T11:00:10.546461Z",
     "shell.execute_reply": "2025-01-26T11:00:10.545718Z",
     "shell.execute_reply.started": "2025-01-26T11:00:09.419830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df['user_id'] = ratings_df['user_id'].map(user_mapping)\n",
    "ratings_df['movie_id'] = ratings_df['movie_id'].map(item_mapping)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:10.547654Z",
     "iopub.status.busy": "2025-01-26T11:00:10.547315Z",
     "iopub.status.idle": "2025-01-26T11:00:10.678338Z",
     "shell.execute_reply": "2025-01-26T11:00:10.677375Z",
     "shell.execute_reply.started": "2025-01-26T11:00:10.547627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.drop(['_id'],axis = 1)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:10.681293Z",
     "iopub.status.busy": "2025-01-26T11:00:10.681005Z",
     "iopub.status.idle": "2025-01-26T11:00:13.559226Z",
     "shell.execute_reply": "2025-01-26T11:00:13.558539Z",
     "shell.execute_reply.started": "2025-01-26T11:00:10.681269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, val = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "train, test = train_test_split(train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:13.561277Z",
     "iopub.status.busy": "2025-01-26T11:00:13.560967Z",
     "iopub.status.idle": "2025-01-26T11:00:13.565579Z",
     "shell.execute_reply": "2025-01-26T11:00:13.564963Z",
     "shell.execute_reply.started": "2025-01-26T11:00:13.561238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_users = len(user_mapping)\n",
    "n_items = len(item_mapping)\n",
    "print(f'No. users : {n_users}, no. items : {n_items}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easiest to construct model using functional API (multiple input network). 3 dense layers of sizes 64,32,16 respectively follow the embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: MLP ONLY MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:13.566747Z",
     "iopub.status.busy": "2025-01-26T11:00:13.566450Z",
     "iopub.status.idle": "2025-01-26T11:00:13.688282Z",
     "shell.execute_reply": "2025-01-26T11:00:13.687632Z",
     "shell.execute_reply.started": "2025-01-26T11:00:13.566716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 32  # Latent factor size\n",
    "mlp_layer_sizes = [64, 32, 16]  # Fully connected layers\n",
    "\n",
    "# 1d input for user and item \n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "# Embedding layers\n",
    "user_embedding = Embedding(n_users, embedding_dim, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_items, embedding_dim, name='item_embedding')(item_input)\n",
    "\n",
    "# Flatten embeddings\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "\n",
    "# Concatenate embeddings\n",
    "concat_vec = Concatenate()([user_vec, item_vec])\n",
    "# MLP layers\n",
    "mlp = concat_vec\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp = Dense(size, activation='relu')(mlp)\n",
    "    mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "# Output layer (e.g., single rating prediction)\n",
    "output = Dense(1, activation='linear', name='output')(mlp)\n",
    "\n",
    "# Build and compile the model\n",
    "ncf_model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "ncf_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "ncf_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:13.689394Z",
     "iopub.status.busy": "2025-01-26T11:00:13.689102Z",
     "iopub.status.idle": "2025-01-26T11:00:13.694186Z",
     "shell.execute_reply": "2025-01-26T11:00:13.693502Z",
     "shell.execute_reply.started": "2025-01-26T11:00:13.689363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:00:13.695752Z",
     "iopub.status.busy": "2025-01-26T11:00:13.695341Z",
     "iopub.status.idle": "2025-01-26T11:00:17.752131Z",
     "shell.execute_reply": "2025-01-26T11:00:17.751205Z",
     "shell.execute_reply.started": "2025-01-26T11:00:13.695691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dict = defaultdict(set)\n",
    "\n",
    "# Assuming your train DataFrame has columns 'user_id' and 'movie_id':\n",
    "for user, item in zip(train['user_id'], train['movie_id']):\n",
    "    train_dict[user].add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T11:52:38.450420Z",
     "iopub.status.busy": "2025-01-26T11:52:38.450121Z",
     "iopub.status.idle": "2025-01-26T11:52:38.522172Z",
     "shell.execute_reply": "2025-01-26T11:52:38.521156Z",
     "shell.execute_reply.started": "2025-01-26T11:52:38.450395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train = [train['user_id'].values, train['movie_id'].values]\n",
    "y_train = train['rating_val'].values\n",
    "x_val = [val['user_id'].values, val['movie_id'].values]\n",
    "y_val = val['rating_val'].values\n",
    "x_test = [test['user_id'].values, test['movie_id'].values]\n",
    "y_test = test['rating_val'].values\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "# Train the model\n",
    "history = ncf_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1,\n",
    "    callbacks = [early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.362249Z",
     "iopub.status.idle": "2025-01-26T11:00:18.362497Z",
     "shell.execute_reply": "2025-01-26T11:00:18.362394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights('/kaggle/input/parquet-dataset/ncf_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.363438Z",
     "iopub.status.idle": "2025-01-26T11:00:18.363834Z",
     "shell.execute_reply": "2025-01-26T11:00:18.363649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss, mae = ncf_model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "predictions = ncf_model.predict(x_test)\n",
    "rmse = sqrt(np.mean((predictions.flatten() - y_test) ** 2))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.365141Z",
     "iopub.status.idle": "2025-01-26T11:00:18.365526Z",
     "shell.execute_reply": "2025-01-26T11:00:18.365356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_id = 'jay'  # Replace with a valid user_id\n",
    "user_idx = user_mapping.get(user_id)\n",
    "print(user_idx)\n",
    "# Predict ratings for all items\n",
    "all_items = np.arange(n_items)\n",
    "user_tensor = np.array([user_idx] * n_items)\n",
    "predicted_ratings = ncf_model.predict([user_tensor, all_items])\n",
    "\n",
    "# Get top 10 recommendations\n",
    "top_items = np.argsort(predicted_ratings.flatten())[::-1][:10]\n",
    "recommended_movie_ids = [list(item_mapping.keys())[list(item_mapping.values()).index(idx)] for idx in top_items]\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movie_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.366290Z",
     "iopub.status.idle": "2025-01-26T11:00:18.366669Z",
     "shell.execute_reply": "2025-01-26T11:00:18.366502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = test.rename(columns={'rating_val': 'prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.367357Z",
     "iopub.status.idle": "2025-01-26T11:00:18.367743Z",
     "shell.execute_reply": "2025-01-26T11:00:18.367559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.368463Z",
     "iopub.status.idle": "2025-01-26T11:00:18.368809Z",
     "shell.execute_reply": "2025-01-26T11:00:18.368652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# all_user_ids = list(user_mapping.keys())       # e.g. original user IDs\n",
    "# all_user_idxs = list(user_mapping.values())    # integer indices\n",
    "# BATCH_SIZE = 512\n",
    "# all_items = np.arange(n_items)\n",
    "\n",
    "# predictions_list = []\n",
    "# for i in range(0, len(all_user_idxs), BATCH_SIZE):\n",
    "#     # Take a chunk of users\n",
    "#     user_idx_batch = all_user_idxs[i : i + BATCH_SIZE]\n",
    "#     user_id_batch = all_user_ids[i : i + BATCH_SIZE]\n",
    "\n",
    "#     # Repeat items for each user in the batch\n",
    "#     # shape: (#users_in_batch * n_items,)\n",
    "#     tile_users = np.repeat(user_idx_batch, n_items)\n",
    "#     tile_items = np.tile(all_items, len(user_idx_batch))\n",
    "\n",
    "#     # Model predict on that entire chunk\n",
    "#     batch_preds = ncf_model.predict([tile_users, tile_items])  \n",
    "\n",
    "#     # Now we map predictions back to (user, item) pairs\n",
    "#     # We'll build a DataFrame\n",
    "#     df_chunk = pd.DataFrame({\n",
    "#         \"userID\": np.repeat(user_id_batch, n_items),\n",
    "#         \"itemID\": tile_items,\n",
    "#         \"prediction\": batch_preds\n",
    "#     })\n",
    "#     predictions_list.append(df_chunk)\n",
    "\n",
    "# predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "# ### 1 HOUR 45 MINS FOR ENTIRE DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.369552Z",
     "iopub.status.idle": "2025-01-26T11:00:18.369944Z",
     "shell.execute_reply": "2025-01-26T11:00:18.369778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.370621Z",
     "iopub.status.idle": "2025-01-26T11:00:18.370966Z",
     "shell.execute_reply": "2025-01-26T11:00:18.370852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",\n",
    "        \"user_id\": \"userID\",\n",
    "        \"rating_val\": \"rating\"  # this is your ground truth rating\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# And ensure predictions_df has the same userID/itemID columns plus 'prediction'\n",
    "predictions_df.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",  # if you had that column\n",
    "        \"user_id\": \"userID\"\n",
    "        # 'prediction' can stay as 'prediction'\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.371567Z",
     "iopub.status.idle": "2025-01-26T11:00:18.371893Z",
     "shell.execute_reply": "2025-01-26T11:00:18.371780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "#eval_map = map(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "print(\"reaches this\")\n",
    "eval_precision = precision_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "print(\"reaches this\")\n",
    "eval_recall = recall_at_k(test, predictions_df, col_prediction='prediction', k=TOP_K)\n",
    "print(\"reaches this\")\n",
    "print(\n",
    "    #f\"MAP:\\t{eval_map:.6f}\",\n",
    "    f\"NDCG:\\t{eval_ndcg:.6f}\",\n",
    "    f\"Precision@K:\\t{eval_precision:.6f}\",\n",
    "    f\"Recall@K:\\t{eval_recall:.6f}\",\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPLEXITY OF PREDICTING FOR ALL USERS AND ITEMS IS (no_users * no_items) -> VERY LARGE. \n",
    "Use negative sampling - take 1 item user interacted with, and 50 or 100 items the user did NOT interact (did not rate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with manual testing for different users, the model basically outputs the highest rated shows and movies that the user has not rated. how to improve on that ? -> bert tokenization of movie descriptions into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.373180Z",
     "iopub.status.idle": "2025-01-26T11:00:18.373500Z",
     "shell.execute_reply": "2025-01-26T11:00:18.373391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.374199Z",
     "iopub.status.idle": "2025-01-26T11:00:18.374495Z",
     "shell.execute_reply": "2025-01-26T11:00:18.374372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for user_id, subset in train.groupby('user_id'):\n",
    "    train_dict[user_id] = set(subset['movie_id'].unique())\n",
    "\n",
    "# Build 'all_items_set' of all unique itemIDs in train + test\n",
    "all_items_set = set(train['movie_id'].unique()) | set(test['itemID'].unique())\n",
    "\n",
    "# Number of items to evaluate\n",
    "k = 10\n",
    "hits = []\n",
    "ndcgs = []\n",
    "\n",
    "unique_user_ids = test['userID'].unique()\n",
    "\n",
    "# We'll limit the loop to the first 100 users \n",
    "for user_id in tqdm(unique_user_ids[:100], desc=\"Processing users\"):\n",
    "    # Suppose each user has exactly 1 item in test to check\n",
    "    test_items = test.loc[test['userID'] == user_id, 'itemID'].values\n",
    "    pos_item = test_items[0]  # if exactly one test item\n",
    "\n",
    "    # Build negative set (exclude train_dict[user_id] + the positive item)\n",
    "    user_train_items = train_dict[user_id] | {pos_item}\n",
    "    possible_negatives = list(all_items_set - user_train_items)\n",
    "\n",
    "    # Sample 99 negatives\n",
    "    neg_items = random.sample(possible_negatives, 99)\n",
    "\n",
    "    # Combine into a batch\n",
    "    item_batch = [pos_item] + neg_items\n",
    "    user_batch = [user_id] * len(item_batch)\n",
    "\n",
    "    # Get predictions (change ncf_model to your actual model)\n",
    "    scores = ncf_model.predict([np.array(user_batch), np.array(item_batch)])\n",
    "    scores = np.squeeze(scores)\n",
    "\n",
    "    # Positive item is at index 0\n",
    "    pos_score = scores[0]\n",
    "    rank = np.sum(scores >= pos_score)  # 1-based rank\n",
    "\n",
    "    if rank <= k:\n",
    "        hits.append(1)\n",
    "        ndcgs.append(1 / np.log2(rank + 1))\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        ndcgs.append(0)\n",
    "\n",
    "# Finally, compute and print\n",
    "hr = np.mean(hits)\n",
    "ndcg = np.mean(ndcgs)\n",
    "print(f\"Hit@{k}: {hr:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: GMF AND MLP INHOUSE MODELS TRAINED SEPARATELY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 : NEU_MF MODEL - GMF + MLP INHOUSE MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3 : NEU_MF MODEL - GMF + MLP FROM RECOMENDERS LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.375202Z",
     "iopub.status.idle": "2025-01-26T11:00:18.375492Z",
     "shell.execute_reply": "2025-01-26T11:00:18.375346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gmf_embed_dim = 16\n",
    "mlp_embed_dim = 16\n",
    "mlp_layer_sizes = [64, 32, 16]\n",
    "\n",
    "# ----- 1) Define two sets of Embeddings for GMF and MLP -----\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "# GMF embeddings\n",
    "gmf_user_embedding = Embedding(n_users, gmf_embed_dim, name='gmf_user_embedding')(user_input)\n",
    "gmf_item_embedding = Embedding(n_items, gmf_embed_dim, name='gmf_item_embedding')(item_input)\n",
    "gmf_user_vec = Flatten()(gmf_user_embedding)\n",
    "gmf_item_vec = Flatten()(gmf_item_embedding)\n",
    "\n",
    "# Element-wise multiply for GMF part\n",
    "gmf_out = Multiply()([gmf_user_vec, gmf_item_vec])\n",
    "\n",
    "# MLP embeddings\n",
    "mlp_user_embedding = Embedding(n_users, mlp_embed_dim, name='mlp_user_embedding')(user_input)\n",
    "mlp_item_embedding = Embedding(n_items, mlp_embed_dim, name='mlp_item_embedding')(item_input)\n",
    "mlp_user_vec = Flatten()(mlp_user_embedding)\n",
    "mlp_item_vec = Flatten()(mlp_item_embedding)\n",
    "\n",
    "# Concatenate for MLP part\n",
    "mlp_vec = Concatenate()([mlp_user_vec, mlp_item_vec])\n",
    "\n",
    "# ----- 2) Pass MLP concat through the hidden layers -----\n",
    "mlp_out = mlp_vec\n",
    "for size in mlp_layer_sizes:\n",
    "    mlp_out = Dense(size, activation='relu')(mlp_out)\n",
    "\n",
    "# ----- 3) Final NeuMF fusion -----\n",
    "fusion = Concatenate()([gmf_out, mlp_out])  # combine GMF & MLP\n",
    "output = Dense(1, activation='sigmoid', name='output')(fusion)\n",
    "\n",
    "neu_mf_model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "neu_mf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "neu_mf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function and output layer can be either binary crossentropy and sigmoid (user interacts or doesn't), or root mean square error and linear output (ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.376239Z",
     "iopub.status.idle": "2025-01-26T11:00:18.376574Z",
     "shell.execute_reply": "2025-01-26T11:00:18.376459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.377250Z",
     "iopub.status.idle": "2025-01-26T11:00:18.377544Z",
     "shell.execute_reply": "2025-01-26T11:00:18.377414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.378219Z",
     "iopub.status.idle": "2025-01-26T11:00:18.378568Z",
     "shell.execute_reply": "2025-01-26T11:00:18.378442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",\n",
    "        \"rating_val\": \"rating\",\n",
    "        \"user_id\": \"userID\"\n",
    "    }\n",
    ")\n",
    "test = test.rename(\n",
    "    columns={\n",
    "        \"prediction\": \"rating\"  \n",
    "    }\n",
    ")\n",
    "# have to be renamed for the recommendations dataset loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.379559Z",
     "iopub.status.idle": "2025-01-26T11:00:18.379880Z",
     "shell.execute_reply": "2025-01-26T11:00:18.379771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.380776Z",
     "iopub.status.idle": "2025-01-26T11:00:18.381072Z",
     "shell.execute_reply": "2025-01-26T11:00:18.380965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ratings = ratings_df.rename(\n",
    "    columns={\n",
    "        \"movie_id\": \"itemID\",\n",
    "        \"rating_val\": \"rating\",\n",
    "        \"user_id\": \"user\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.381572Z",
     "iopub.status.idle": "2025-01-26T11:00:18.381843Z",
     "shell.execute_reply": "2025-01-26T11:00:18.381738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = python_chrono_split(merged_df, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.382403Z",
     "iopub.status.idle": "2025-01-26T11:00:18.382634Z",
     "shell.execute_reply": "2025-01-26T11:00:18.382538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df[test_df[\"userID\"].isin(train_df[\"userID\"].unique())]\n",
    "test_df = test_df[test_df[\"itemID\"].isin(train_df[\"itemID\"].unique())]\n",
    "train_df = train_df.sort_values(['userID', 'timestamp']).reset_index(drop=True)\n",
    "test_df  = test_df.sort_values(['userID', 'timestamp']).reset_index(drop=True)\n",
    "# 2) Create a leave-one-out test set by taking last row for user\n",
    "leave_one_out_test = test_df.groupby(\"userID\").last().reset_index()\n",
    "\n",
    "# 3) Write them to CSV files\n",
    "train_file = \"./train2.csv\"\n",
    "test_file = \"./test2.csv\"\n",
    "leave_one_out_test_file = \"./leave_one_out_test2.csv\"\n",
    "\n",
    "train.to_csv(train_file, index=False)\n",
    "test.to_csv(test_file, index=False)\n",
    "leave_one_out_test.to_csv(leave_one_out_test_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.383345Z",
     "iopub.status.idle": "2025-01-26T11:00:18.383584Z",
     "shell.execute_reply": "2025-01-26T11:00:18.383485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.384282Z",
     "iopub.status.idle": "2025-01-26T11:00:18.384605Z",
     "shell.execute_reply": "2025-01-26T11:00:18.384432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_users = merged_df['userID'].unique()\n",
    "user_map = {user: idx for idx, user in enumerate(unique_users)}\n",
    "merged_df['userID_num'] = merged_df['userID'].map(user_map)\n",
    "\n",
    "# b. Convert `itemID` to numeric\n",
    "unique_items = merged_df['itemID'].unique()\n",
    "item_map = {item: idx for idx, item in enumerate(unique_items)}\n",
    "merged_df['itemID_num'] = merged_df['itemID'].map(item_map)\n",
    "\n",
    "# Optional: Save the mappings for future reference\n",
    "with open('user_map.json', 'w') as f:\n",
    "    json.dump(user_map, f)\n",
    "with open('item_map.json', 'w') as f:\n",
    "    json.dump(item_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.385393Z",
     "iopub.status.idle": "2025-01-26T11:00:18.385711Z",
     "shell.execute_reply": "2025-01-26T11:00:18.385576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid timestamps if any\n",
    "merged_df = merged_df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Sort by `userID_num` and `timestamp`\n",
    "merged_df = merged_df.sort_values(['userID_num', 'timestamp']).reset_index(drop=True)\n",
    "assert merged_df['userID_num'].isnull().sum() == 0, \"Some userIDs were not mapped correctly.\"\n",
    "assert merged_df['itemID_num'].isnull().sum() == 0, \"Some itemIDs were not mapped correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.386355Z",
     "iopub.status.idle": "2025-01-26T11:00:18.386658Z",
     "shell.execute_reply": "2025-01-26T11:00:18.386525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = python_chrono_split(merged_df, 0.75)\n",
    "train_df_to_save = train_df[['userID_num', 'itemID_num', 'rating', 'timestamp']]\n",
    "test_df_to_save = test_df[['userID_num', 'itemID_num', 'rating', 'timestamp']]\n",
    "\n",
    "# Optionally, rename columns to match expected names\n",
    "train_df_to_save.rename(columns={\n",
    "    'userID_num': 'userID',\n",
    "    'itemID_num': 'itemID'\n",
    "}, inplace=True)\n",
    "test_df_to_save.rename(columns={\n",
    "    'userID_num': 'userID',\n",
    "    'itemID_num': 'itemID'\n",
    "}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "train_file = 'train_data.csv'\n",
    "test_file = 'test_data.csv'\n",
    "\n",
    "train_df_to_save.to_csv(train_file, index=False)\n",
    "test_df_to_save.to_csv(test_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.387387Z",
     "iopub.status.idle": "2025-01-26T11:00:18.387666Z",
     "shell.execute_reply": "2025-01-26T11:00:18.387555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(['userID_num', 'timestamp']).reset_index(drop=True)\n",
    "test_df = test_df.sort_values(['userID_num', 'timestamp']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.388369Z",
     "iopub.status.idle": "2025-01-26T11:00:18.388675Z",
     "shell.execute_reply": "2025-01-26T11:00:18.388541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['itemID'],axis = 1)\n",
    "train_df = train_df.drop(['userID'],axis = 1)\n",
    "test_df = test_df.drop(['itemID'],axis = 1)\n",
    "test_df = test_df.drop(['userID'],axis = 1)\n",
    "# not sure whether test needs to be dropping itemID actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.389390Z",
     "iopub.status.idle": "2025-01-26T11:00:18.389663Z",
     "shell.execute_reply": "2025-01-26T11:00:18.389547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.390548Z",
     "iopub.status.idle": "2025-01-26T11:00:18.390873Z",
     "shell.execute_reply": "2025-01-26T11:00:18.390761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.rename(columns={\n",
    "    'userID_num': 'userID',\n",
    "    'itemID_num': 'itemID'\n",
    "}, inplace=True)\n",
    "test_df.rename(columns={\n",
    "    'userID_num': 'userID',\n",
    "    'itemID_num': 'itemID'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.391562Z",
     "iopub.status.idle": "2025-01-26T11:00:18.391906Z",
     "shell.execute_reply": "2025-01-26T11:00:18.391767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.392782Z",
     "iopub.status.idle": "2025-01-26T11:00:18.393096Z",
     "shell.execute_reply": "2025-01-26T11:00:18.392921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_file_fixed = 'train_file_fixed.csv'\n",
    "test_file_fixed = 'test_file_fixed.csv'\n",
    "leave_one_out_fixed_file = 'leave_one_out_fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.393703Z",
     "iopub.status.idle": "2025-01-26T11:00:18.394033Z",
     "shell.execute_reply": "2025-01-26T11:00:18.393918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(['userID', 'timestamp']).reset_index(drop=True)\n",
    "test_df  = test_df.sort_values(['userID', 'timestamp']).reset_index(drop=True)\n",
    "leave_one_out_fixed = test_df.groupby(\"userID\").last().reset_index()\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "train_df.to_csv(train_file_fixed, index=False)\n",
    "test_df.to_csv(test_file_fixed, index=False)\n",
    "leave_one_out_fixed.to_csv(leave_one_out_fixed_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.394727Z",
     "iopub.status.idle": "2025-01-26T11:00:18.395016Z",
     "shell.execute_reply": "2025-01-26T11:00:18.394908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.395693Z",
     "iopub.status.idle": "2025-01-26T11:00:18.395986Z",
     "shell.execute_reply": "2025-01-26T11:00:18.395879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.396899Z",
     "iopub.status.idle": "2025-01-26T11:00:18.397162Z",
     "shell.execute_reply": "2025-01-26T11:00:18.397057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "    data = NCFDataset(\n",
    "        train_file=train_file_fixed,\n",
    "        test_file=leave_one_out_fixed_file,\n",
    "        seed=42,\n",
    "        overwrite_test_file_full=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.397923Z",
     "iopub.status.idle": "2025-01-26T11:00:18.398205Z",
     "shell.execute_reply": "2025-01-26T11:00:18.398075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5 \n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.399239Z",
     "iopub.status.idle": "2025-01-26T11:00:18.399567Z",
     "shell.execute_reply": "2025-01-26T11:00:18.399408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TqdmCallback(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.pbar = tqdm(total=self.epochs, desc='Training', unit='epoch')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.400427Z",
     "iopub.status.idle": "2025-01-26T11:00:18.400758Z",
     "shell.execute_reply": "2025-01-26T11:00:18.400595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    model = NCF(\n",
    "        n_users=data.n_users, \n",
    "        n_items=data.n_items,\n",
    "        model_type=\"NeuMF\",\n",
    "        n_factors=4,\n",
    "        layer_sizes=[16,8,4],\n",
    "        n_epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=1e-3,\n",
    "        verbose=1,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.401617Z",
     "iopub.status.idle": "2025-01-26T11:00:18.401923Z",
     "shell.execute_reply": "2025-01-26T11:00:18.401806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    num_batches = int(np.ceil(len(data.train) / BATCH_SIZE))\n",
    "    \n",
    "    # Initialize tqdm progress bar for batches\n",
    "    with tqdm(total=num_batches, desc=f'Epoch {epoch}/{EPOCHS}', unit='batch') as pbar:\n",
    "        for batch in data.get_batches(BATCH_SIZE):\n",
    "            # Extract inputs and targets from the batch\n",
    "            user_ids, item_ids, labels = batch\n",
    "            \n",
    "            # Train on the current batch\n",
    "            loss, accuracy = model.train_on_batch(user_ids, item_ids, labels)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss, 'accuracy': accuracy})\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Calculate average metrics for the epoch\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_accuracy = epoch_accuracy / num_batches\n",
    "    \n",
    "    # Display epoch summary\n",
    "    print(f'Epoch {epoch}/{EPOCHS} - Loss: {avg_loss:.4f} - Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.402581Z",
     "iopub.status.idle": "2025-01-26T11:00:18.402901Z",
     "shell.execute_reply": "2025-01-26T11:00:18.402768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.403667Z",
     "iopub.status.idle": "2025-01-26T11:00:18.403946Z",
     "shell.execute_reply": "2025-01-26T11:00:18.403845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(user_ids, item_ids, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([user_ids, item_ids], training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.404513Z",
     "iopub.status.idle": "2025-01-26T11:00:18.404786Z",
     "shell.execute_reply": "2025-01-26T11:00:18.404648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-26T11:00:18.405638Z",
     "iopub.status.idle": "2025-01-26T11:00:18.405970Z",
     "shell.execute_reply": "2025-01-26T11:00:18.405854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Reset the metrics at the start of the epoch\n",
    "    \n",
    "    # Initialize tqdm progress bar for batches\n",
    "    with tqdm(total=steps_per_epoch, desc=f'Epoch {epoch}/{EPOCHS}', unit='batch') as pbar:\n",
    "        for batch in batched_dataset:\n",
    "            user_ids, item_ids, labels = batch  # Adjust based on your batch structure\n",
    "            \n",
    "            # Perform a training step\n",
    "            train_step(user_ids, item_ids, labels)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{train_loss.result():.4f}\",\n",
    "                'accuracy': f\"{train_accuracy.result():.4f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Display epoch metrics\n",
    "    print(f'Epoch {epoch}/{EPOCHS} - Loss: {train_loss.result():.4f} - Accuracy: {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1300195,
     "sourceId": 3341890,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6442594,
     "sourceId": 10397826,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
